"""
å®ç°æœ€ä½³èšç±»æ–¹æ³•
ç‰¹å¾ç»„åˆ2: æ‰€å¾—_median_household_income, medical_index, tertiary_industry_ratio
ç»“æœ: Silhouette=0.613, ä¸€è‡´æ€§=0.836

è¿™ä¸ªç»„åˆé€‰æ‹©äº†æœ€å…·ä»£è¡¨æ€§çš„ä¸‰ä¸ªç‰¹å¾ï¼š
1. ç»æµæŒ‡æ ‡ï¼šå®¶åº­æ”¶å…¥ä¸­ä½æ•°
2. æœåŠ¡æŒ‡æ ‡ï¼šåŒ»ç–—æœåŠ¡æŒ‡æ•°  
3. äº§ä¸šæŒ‡æ ‡ï¼šç¬¬ä¸‰äº§ä¸šæ¯”ä¾‹
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings
import os
import matplotlib

warnings.filterwarnings('ignore')

# è®¾å®šä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

OUTPUT_DIR = 'output'
RANDOM_STATE = 42

def load_and_prepare_data():
    """è½½å…¥å¹¶å‡†å¤‡æœ€ä½³ç‰¹å¾ç»„åˆæ•°æ®"""
    print("ğŸ“‚ è½½å…¥æ•°æ®å¹¶å‡†å¤‡æœ€ä½³ç‰¹å¾ç»„åˆ...")
    
    # è½½å…¥åŸå§‹æ•°æ®
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    districts = df['å€åŸŸåˆ¥'].tolist()
    
    # æœ€ä½³ç‰¹å¾ç»„åˆ2: 3ä¸ªæ ¸å¿ƒç‰¹å¾
    optimal_features = [
        'æ‰€å¾—_median_household_income',  # ç»æµæ°´å¹³
        'medical_index',                # åŒ»ç–—æœåŠ¡
        'tertiary_industry_ratio'       # äº§ä¸šç»“æ„
    ]
    
    print(f"âœ… æœ€ä½³ç‰¹å¾ç»„åˆ: {', '.join(optimal_features)}")
    
    # æå–ç‰¹å¾æ•°æ®
    X_optimal = df[optimal_features].values
    
    print(f"æ•°æ®ç»´åº¦: {X_optimal.shape}")
    print(f"è¡Œæ”¿åŒºæ•°é‡: {len(districts)}")
    
    return df, X_optimal, districts, optimal_features

def calculate_potential_score_optimal(df):
    """ä½¿ç”¨æœ€ä½³ç‰¹å¾è®¡ç®—æ½œåŠ›ç»¼åˆåˆ†æ•°"""
    print("\nğŸ“Š è®¡ç®—æ½œåŠ›ç»¼åˆåˆ†æ•° (æœ€ä½³ç‰¹å¾)...")
    
    # æ½œåŠ›æŒ‡æ ‡ï¼ˆä¸èšç±»ç‰¹å¾ä¸€è‡´ä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼‰
    potential_indicators = [
        'æ‰€å¾—_median_household_income',
        'medical_index', 
        'tertiary_industry_ratio',
        'äººå£_working_age_ratio'  # é¢å¤–æ·»åŠ äººå£ç»“æ„æŒ‡æ ‡
    ]
    
    # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å­˜åœ¨
    available_indicators = [col for col in potential_indicators if col in df.columns]
    print(f"  ä½¿ç”¨æ½œåŠ›æŒ‡æ ‡: {', '.join(available_indicators)}")
    
    # å¯¹é€‰å®šæŒ‡æ ‡è¿›è¡Œz-scoreæ ‡å‡†åŒ–
    z_scores = pd.DataFrame()
    z_scores['å€åŸŸåˆ¥'] = df['å€åŸŸåˆ¥']
    
    for indicator in available_indicators:
        z_score = stats.zscore(df[indicator])
        z_scores[f'{indicator}_zscore'] = z_score
        print(f"  {indicator}: å¹³å‡={df[indicator].mean():.2f}, æ ‡å‡†å·®={df[indicator].std():.2f}")
    
    # è®¡ç®—æ½œåŠ›ç»¼åˆåˆ†æ•°ï¼ˆz-scoreçš„å¹³å‡ï¼‰
    z_score_cols = [col for col in z_scores.columns if col.endswith('_zscore')]
    z_scores['potential_score'] = z_scores[z_score_cols].mean(axis=1)
    
    print(f"âœ… æ½œåŠ›ç»¼åˆåˆ†æ•°è®¡ç®—å®Œæˆ")
    print(f"  åˆ†æ•°èŒƒå›´: {z_scores['potential_score'].min():.3f} ~ {z_scores['potential_score'].max():.3f}")
    
    return z_scores

def assign_district_labels_optimal(potential_scores):
    """åˆ†é…è¡Œæ”¿åŒºæ ‡ç­¾"""
    print("\nğŸ·ï¸ åˆ†é…è¡Œæ”¿åŒºå‘å±•æ½œåŠ›æ ‡ç­¾...")
    
    # ä½¿ç”¨qcutæŒ‰åˆ†ä½æ•°åˆ‡æˆä¸‰ç­‰ä»½
    district_labels = pd.qcut(
        potential_scores['potential_score'], 
        q=3, 
        labels=['ä½æ½›åŠ›', 'ä¸­æ½›åŠ›', 'é«˜æ½›åŠ›']
    )
    
    potential_scores['district_label'] = district_labels
    
    # æ˜¾ç¤ºåˆ†ç»„ç»“æœ
    for label in ['ä½æ½›åŠ›', 'ä¸­æ½›åŠ›', 'é«˜æ½›åŠ›']:
        districts_in_group = potential_scores[potential_scores['district_label'] == label]['å€åŸŸåˆ¥'].tolist()
        score_range = potential_scores[potential_scores['district_label'] == label]['potential_score']
        print(f"  {label}: {len(districts_in_group)} ä¸ªè¡Œæ”¿åŒº")
        print(f"    è¡Œæ”¿åŒº: {', '.join(districts_in_group)}")
        print(f"    æ½œåŠ›åˆ†æ•°èŒƒå›´: {score_range.min():.3f} ~ {score_range.max():.3f}")
    
    return potential_scores

def perform_optimal_clustering(X_optimal, districts, df):
    """æ‰§è¡Œæœ€ä½³èšç±»æ–¹æ³•"""
    print("\nğŸ” æ‰§è¡Œæœ€ä½³èšç±»æ–¹æ³•...")
    print("æ–¹æ³•: 3æ ¸å¿ƒç‰¹å¾ + t-SNEé™ç»´ + Wardå±‚æ¬¡èšç±»")
    
    # t-SNEé™ç»´ï¼ˆä½¿ç”¨æœ€ä½³ç‰¹å¾ï¼‰
    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
    X_tsne = tsne.fit_transform(X_optimal)
    print(f"âœ… t-SNEé™ç»´å®Œæˆ: {X_optimal.shape} â†’ {X_tsne.shape}")
    
    # Wardå±‚æ¬¡èšç±»
    ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
    cluster_labels = ward.fit_predict(X_tsne)
    
    # è®¡ç®—Silhouetteåˆ†æ•°
    silhouette = silhouette_score(X_tsne, cluster_labels)
    print(f"âœ… Wardå±‚æ¬¡èšç±»å®Œæˆ: 3ä¸ªé›†ç¾¤, Silhouetteåˆ†æ•°={silhouette:.3f}")
    
    # è®¡ç®—æ½œåŠ›åˆ†æ•°å’Œè¯­ä¹‰ä¸€è‡´æ€§
    potential_scores = calculate_potential_score_optimal(df)
    district_labels_df = assign_district_labels_optimal(potential_scores)
    
    # ä½¿ç”¨å¤šæ•°å†³åˆ†é…é›†ç¾¤æ ‡ç­¾
    cluster_potential_mapping = assign_cluster_labels_by_majority(cluster_labels, district_labels_df, districts)
    
    # è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§
    consistency_score = evaluate_semantic_consistency(cluster_labels, district_labels_df, districts)
    
    print(f"âœ… è¯­ä¹‰ä¸€è‡´æ€§: {consistency_score:.3f}")
    print(f"ğŸ¯ ç»¼åˆè¡¨ç°: Silhouette={silhouette:.3f}, ä¸€è‡´æ€§={consistency_score:.3f}")
    
    # åˆ›å»ºç»“æœDataFrame
    results_df = pd.DataFrame({
        'è¡Œæ”¿åŒº': districts,
        'é›†ç¾¤ç¼–å·': cluster_labels,
        'æ½œåŠ›ç­‰çº§': [cluster_potential_mapping[label] for label in cluster_labels],
        'tsne_x': X_tsne[:, 0],
        'tsne_y': X_tsne[:, 1]
    })
    
    # æ·»åŠ æ½œåŠ›åˆ†æ•°ä¿¡æ¯
    potential_score_dict = dict(zip(district_labels_df['å€åŸŸåˆ¥'], district_labels_df['potential_score']))
    results_df['æ½œåŠ›åˆ†æ•°'] = [potential_score_dict[district] for district in districts]
    
    # æ·»åŠ åŸå§‹ç‰¹å¾å€¼
    for i, feature in enumerate(['å®¶åº­æ”¶å…¥ä¸­ä½æ•°', 'åŒ»ç–—æŒ‡æ•°', 'ç¬¬ä¸‰äº§ä¸šæ¯”ä¾‹']):
        results_df[feature] = X_optimal[:, i]
    
    return results_df, X_tsne, cluster_labels, silhouette, consistency_score

def assign_cluster_labels_by_majority(cluster_labels, district_labels_df, districts):
    """ä½¿ç”¨å¤šæ•°å†³åˆ†é…é›†ç¾¤æ ‡ç­¾"""
    mapping_df = pd.DataFrame({
        'cluster_id': cluster_labels,
        'district_label': district_labels_df['district_label'].values,
        'district': districts
    })
    
    cluster_potential_mapping = {}
    for cluster_id in range(3):
        cluster_data = mapping_df[mapping_df['cluster_id'] == cluster_id]
        label_counts = cluster_data['district_label'].value_counts()
        majority_label = label_counts.index[0]
        cluster_potential_mapping[cluster_id] = majority_label
        
        print(f"  é›†ç¾¤ {cluster_id}: {majority_label}")
        print(f"    æˆå‘˜: {', '.join(cluster_data['district'].tolist())}")
    
    return cluster_potential_mapping

def evaluate_semantic_consistency(cluster_labels, district_labels_df, districts):
    """è¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§"""
    mapping_df = pd.DataFrame({
        'cluster_id': cluster_labels,
        'potential_score': district_labels_df['potential_score'].values
    })
    
    # è®¡ç®—å„é›†ç¾¤æ½œåŠ›åˆ†æ•°ç»Ÿè®¡
    consistency_scores = []
    for cluster_id in range(3):
        cluster_data = mapping_df[mapping_df['cluster_id'] == cluster_id]
        potential_scores = cluster_data['potential_score'].values
        
        if len(potential_scores) > 1:
            std = potential_scores.std()
            consistency = 1 / (1 + std)
        else:
            consistency = 1.0
        
        consistency_scores.append(consistency)
    
    return np.mean(consistency_scores)

def create_optimal_visualization(results_df, X_tsne):
    """åˆ›å»ºæœ€ä½³èšç±»ç»“æœçš„è§†è§‰åŒ–"""
    print("\nğŸ¨ åˆ›å»ºæœ€ä½³èšç±»è§†è§‰åŒ–...")
    
    # é¢œè‰²æ˜ å°„
    level_colors = {'é«˜æ½›åŠ›': '#e74c3c', 'ä¸­æ½›åŠ›': '#f39c12', 'ä½æ½›åŠ›': '#3498db'}
    
    # åˆ›å»ºç”»å¸ƒ
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½œåŠ›ç­‰çº§'] == level]
        if len(level_data) > 0:
            plt.scatter(
                level_data['tsne_x'], level_data['tsne_y'],
                c=level_colors.get(level, 'gray'),
                label=level,
                s=150,
                alpha=0.8,
                edgecolors='white',
                linewidth=2
            )
    
    # æ ‡æ³¨è¡Œæ”¿åŒºåç§°
    for i, row in results_df.iterrows():
        plt.annotate(
            row['è¡Œæ”¿åŒº'],
            (row['tsne_x'], row['tsne_y']),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=11,
            ha='left',
            fontweight='bold'
        )
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('æ¡ƒå›­å¸‚è¡Œæ”¿åŒºåˆ†ç¾¤ç»“æœ (æœ€ä½³æ–¹æ³•: 3æ ¸å¿ƒç‰¹å¾)', fontsize=16, fontweight='bold', pad=20)
    plt.legend(fontsize=12, title='å‘å±•æ½œåŠ›ç­‰çº§', title_fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlabel('t-SNE ç»´åº¦ 1', fontsize=12)
    plt.ylabel('t-SNE ç»´åº¦ 2', fontsize=12)
    
    # ä¿å­˜å›¾ç‰‡
    viz_path = os.path.join(OUTPUT_DIR, 'optimal_clustering_visualization.png')
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… æœ€ä½³èšç±»è§†è§‰åŒ–å·²ä¿å­˜: {viz_path}")
    
    return viz_path

def save_optimal_results(results_df):
    """ä¿å­˜æœ€ä½³èšç±»ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜æœ€ä½³èšç±»ç»“æœ...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜èšç±»ç»“æœCSV
    output_path = os.path.join(OUTPUT_DIR, 'optimal_clustering_results.csv')
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ… æœ€ä½³èšç±»ç»“æœå·²ä¿å­˜: {output_path}")
    
    # æ˜¾ç¤ºç»“æœé¢„è§ˆ
    print(f"\nğŸ“‹ ç»“æœé¢„è§ˆ:")
    print(results_df[['è¡Œæ”¿åŒº', 'æ½œåŠ›ç­‰çº§', 'é›†ç¾¤ç¼–å·', 'æ½œåŠ›åˆ†æ•°']].to_string(index=False))
    
    return output_path

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸš€ å®ç°æœ€ä½³èšç±»æ–¹æ³•")
    print("ç‰¹å¾ç»„åˆ2: æ‰€å¾—+åŒ»ç–—+ç¬¬ä¸‰äº§ä¸š")
    print("ç›®æ ‡: Silhouette > 0.6 ä¸” ä¸€è‡´æ€§ > 0.7")
    print("="*60)
    
    # 1. è½½å…¥å¹¶å‡†å¤‡æ•°æ®
    df, X_optimal, districts, optimal_features = load_and_prepare_data()
    
    # 2. æ‰§è¡Œæœ€ä½³èšç±»
    results_df, X_tsne, cluster_labels, silhouette, consistency = perform_optimal_clustering(X_optimal, districts, df)
    
    # 3. åˆ›å»ºè§†è§‰åŒ–
    viz_path = create_optimal_visualization(results_df, X_tsne)
    
    # 4. ä¿å­˜ç»“æœ
    output_path = save_optimal_results(results_df)
    
    # 5. æ€»ç»“
    print(f"\nâœ… æœ€ä½³èšç±»æ–¹æ³•å®ç°å®Œæˆ!")
    print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  - Silhouetteåˆ†æ•°: {silhouette:.3f} {'âœ…' if silhouette > 0.6 else 'âŒ'}")
    print(f"  - è¯­ä¹‰ä¸€è‡´æ€§: {consistency:.3f} {'âœ…' if consistency > 0.7 else 'âŒ'}")
    print(f"  - åˆ†ç¾¤æ•°é‡: 3 âœ…")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - èšç±»ç»“æœ: {output_path}")
    print(f"  - è§†è§‰åŒ–å›¾: {viz_path}")

if __name__ == "__main__":
    main() 