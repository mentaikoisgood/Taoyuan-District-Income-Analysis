"""
é«˜çº§èšç±»ä¼˜åŒ–å®éªŒ
ç›®æ ‡: Silhouette > 0.6 ä¸” è¯­ä¹‰ä¸€è‡´æ€§ > 0.7 ä¸” åˆ†ç¾¤æ•° >= 3

æµ‹è¯•ç­–ç•¥:
1. å¤šç§èšç±»ç®—æ³•ç»„åˆ
2. ç‰¹å¾é€‰æ‹©ä¼˜åŒ–
3. é™ç»´æ–¹æ³•å¯¹æ¯”
4. å‚æ•°è°ƒä¼˜
5. é›†æˆæ–¹æ³•
"""

import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from scipy import stats
import itertools
import warnings
warnings.filterwarnings('ignore')

RANDOM_STATE = 42

def load_data():
    """è½½å…¥æ•°æ®"""
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    districts = df['å€åŸŸåˆ¥'].tolist()
    
    # æ‰€æœ‰å¯ç”¨ç‰¹å¾
    all_features = df.columns[1:].tolist()
    X_full = df.drop('å€åŸŸåˆ¥', axis=1).values
    
    return X_full, districts, all_features, df

def calculate_potential_score(df, features_subset=None):
    """è®¡ç®—æ½œåŠ›ç»¼åˆåˆ†æ•°"""
    if features_subset is None:
        potential_indicators = [
            'äººå£_working_age_ratio',
            'æ‰€å¾—_median_household_income', 
            'tertiary_industry_ratio',
            'medical_index'
        ]
    else:
        potential_indicators = features_subset
    
    # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å­˜åœ¨
    available_indicators = [col for col in potential_indicators if col in df.columns]
    
    # è®¡ç®—z-score
    z_scores = pd.DataFrame()
    z_scores['å€åŸŸåˆ¥'] = df['å€åŸŸåˆ¥']
    
    for indicator in available_indicators:
        z_score = stats.zscore(df[indicator])
        z_scores[f'{indicator}_zscore'] = z_score
    
    # è®¡ç®—æ½œåŠ›ç»¼åˆåˆ†æ•°
    z_score_cols = [col for col in z_scores.columns if col.endswith('_zscore')]
    z_scores['potential_score'] = z_scores[z_score_cols].mean(axis=1)
    
    return z_scores

def assign_district_labels(potential_scores):
    """åˆ†é…è¡Œæ”¿åŒºæ ‡ç­¾"""
    district_labels = pd.qcut(
        potential_scores['potential_score'], 
        q=3, 
        labels=['ä½æ½›åŠ›', 'ä¸­æ½›åŠ›', 'é«˜æ½›åŠ›']
    )
    potential_scores['district_label'] = district_labels
    return potential_scores

def evaluate_semantic_consistency(cluster_labels, district_labels_df, districts):
    """è¯„ä¼°è¯­ä¹‰ä¸€è‡´æ€§"""
    # åˆ›å»ºæ˜ å°„DataFrame
    mapping_df = pd.DataFrame({
        'cluster_id': cluster_labels,
        'district_label': district_labels_df['district_label'].values,
        'district': districts,
        'potential_score': district_labels_df['potential_score'].values
    })
    
    # ä½¿ç”¨å¤šæ•°å†³åˆ†é…é›†ç¾¤æ ‡ç­¾
    cluster_potential_mapping = {}
    for cluster_id in range(len(np.unique(cluster_labels))):
        cluster_data = mapping_df[mapping_df['cluster_id'] == cluster_id]
        if len(cluster_data) > 0:
            label_counts = cluster_data['district_label'].value_counts()
            majority_label = label_counts.index[0]
            cluster_potential_mapping[cluster_id] = majority_label
    
    # è®¡ç®—å„é›†ç¾¤æ½œåŠ›åˆ†æ•°ç»Ÿè®¡
    cluster_stats = {}
    for cluster_id in range(len(np.unique(cluster_labels))):
        cluster_data = mapping_df[mapping_df['cluster_id'] == cluster_id]
        if len(cluster_data) > 0:
            potential_scores = cluster_data['potential_score'].values
            cluster_stats[cluster_id] = {
                'mean_score': potential_scores.mean(),
                'std_score': potential_scores.std() if len(potential_scores) > 1 else 0,
                'count': len(cluster_data)
            }
    
    # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
    consistency_scores = []
    for cluster_id in cluster_stats:
        std = cluster_stats[cluster_id]['std_score']
        consistency = 1 / (1 + std)
        consistency_scores.append(consistency)
    
    overall_consistency = np.mean(consistency_scores) if consistency_scores else 0
    
    return overall_consistency, cluster_potential_mapping, cluster_stats

class ClusteringExperiment:
    def __init__(self, X, districts, df):
        self.X = X
        self.districts = districts
        self.df = df
        self.results = []
    
    def experiment_1_algorithm_comparison(self):
        """å®éªŒ1: ä¸åŒèšç±»ç®—æ³•å¯¹æ¯”"""
        print("ğŸ”¬ å®éªŒ1: èšç±»ç®—æ³•å¯¹æ¯”")
        
        # t-SNEé™ç»´
        tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
        X_tsne = tsne.fit_transform(self.X)
        
        algorithms = [
            ('Ward', AgglomerativeClustering(n_clusters=3, linkage='ward')),
            ('Complete', AgglomerativeClustering(n_clusters=3, linkage='complete')), 
            ('Average', AgglomerativeClustering(n_clusters=3, linkage='average')),
            ('KMeans', KMeans(n_clusters=3, random_state=RANDOM_STATE)),
            ('GMM', GaussianMixture(n_components=3, random_state=RANDOM_STATE))
        ]
        
        for name, algorithm in algorithms:
            if name == 'GMM':
                cluster_labels = algorithm.fit_predict(X_tsne)
            else:
                cluster_labels = algorithm.fit_predict(X_tsne)
            
            if len(np.unique(cluster_labels)) >= 3:
                silhouette = silhouette_score(X_tsne, cluster_labels)
                
                # è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§
                potential_scores = calculate_potential_score(self.df)
                district_labels_df = assign_district_labels(potential_scores)
                consistency, _, _ = evaluate_semantic_consistency(cluster_labels, district_labels_df, self.districts)
                
                self.results.append({
                    'method': f't-SNE + {name}',
                    'silhouette': silhouette,
                    'consistency': consistency,
                    'n_clusters': len(np.unique(cluster_labels)),
                    'details': f'ç®—æ³•: {name}'
                })
                
                print(f"  {name:10s}: Silhouette={silhouette:.3f}, ä¸€è‡´æ€§={consistency:.3f}")
    
    def experiment_2_dimensionality_reduction(self):
        """å®éªŒ2: ä¸åŒé™ç»´æ–¹æ³•"""
        print("\nğŸ”¬ å®éªŒ2: é™ç»´æ–¹æ³•å¯¹æ¯”")
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        
        reduction_methods = [
            ('t-SNE_p3', TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)),
            ('t-SNE_p5', TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=5, max_iter=1000)),
            ('PCA', PCA(n_components=2, random_state=RANDOM_STATE)),
        ]
        
        for name, reducer in reduction_methods:
            X_reduced = reducer.fit_transform(X_scaled)
            
            # ä½¿ç”¨Wardèšç±»
            ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
            cluster_labels = ward.fit_predict(X_reduced)
            
            silhouette = silhouette_score(X_reduced, cluster_labels)
            
            potential_scores = calculate_potential_score(self.df)
            district_labels_df = assign_district_labels(potential_scores)
            consistency, _, _ = evaluate_semantic_consistency(cluster_labels, district_labels_df, self.districts)
            
            self.results.append({
                'method': f'{name} + Ward',
                'silhouette': silhouette,
                'consistency': consistency,
                'n_clusters': len(np.unique(cluster_labels)),
                'details': f'é™ç»´: {name}'
            })
            
            print(f"  {name:10s}: Silhouette={silhouette:.3f}, ä¸€è‡´æ€§={consistency:.3f}")
    
    def experiment_3_feature_selection(self):
        """å®éªŒ3: ç‰¹å¾é€‰æ‹©ä¼˜åŒ–"""
        print("\nğŸ”¬ å®éªŒ3: ç‰¹å¾é€‰æ‹©ä¼˜åŒ–")
        
        # å®šä¹‰ä¸åŒçš„ç‰¹å¾ç»„åˆ
        feature_combinations = [
            # åŸå§‹ç»„åˆ
            ['äººå£_working_age_ratio', 'æ‰€å¾—_median_household_income', 'tertiary_industry_ratio', 'medical_index', 'å•†æ¥­_concentration_index'],
            
            # é«˜ç›¸å…³æ€§ç»„åˆ
            ['æ‰€å¾—_median_household_income', 'medical_index', 'tertiary_industry_ratio'],
            
            # å¹³è¡¡ç»„åˆ
            ['äººå£_working_age_ratio', 'æ‰€å¾—_median_household_income', 'å•†æ¥­_concentration_index', 'medical_index'],
            
            # å…¨ç‰¹å¾
            self.df.columns[1:].tolist()
        ]
        
        for i, features in enumerate(feature_combinations):
            # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
            available_features = [f for f in features if f in self.df.columns]
            if len(available_features) < 3:
                continue
                
            # æå–ç‰¹å¾å­é›†
            feature_indices = [self.df.columns.get_loc(f) - 1 for f in available_features]
            X_subset = self.X[:, feature_indices]
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_subset)
            
            # t-SNEé™ç»´
            tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
            X_reduced = tsne.fit_transform(X_scaled)
            
            # Wardèšç±»
            ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
            cluster_labels = ward.fit_predict(X_reduced)
            
            silhouette = silhouette_score(X_reduced, cluster_labels)
            
            # ä½¿ç”¨ç‰¹å¾å­é›†è®¡ç®—æ½œåŠ›åˆ†æ•°
            potential_indicators = [f for f in available_features if f in ['äººå£_working_age_ratio', 'æ‰€å¾—_median_household_income', 'tertiary_industry_ratio', 'medical_index']]
            potential_scores = calculate_potential_score(self.df, potential_indicators if potential_indicators else available_features[:4])
            district_labels_df = assign_district_labels(potential_scores)
            consistency, _, _ = evaluate_semantic_consistency(cluster_labels, district_labels_df, self.districts)
            
            self.results.append({
                'method': f'ç‰¹å¾ç»„åˆ{i+1}',
                'silhouette': silhouette,
                'consistency': consistency,
                'n_clusters': len(np.unique(cluster_labels)),
                'details': f'ç‰¹å¾æ•°: {len(available_features)}'
            })
            
            print(f"  ç»„åˆ{i+1:2d}: Silhouette={silhouette:.3f}, ä¸€è‡´æ€§={consistency:.3f} (ç‰¹å¾æ•°:{len(available_features)})")
    
    def experiment_4_scaling_methods(self):
        """å®éªŒ4: ä¸åŒæ ‡å‡†åŒ–æ–¹æ³•"""
        print("\nğŸ”¬ å®éªŒ4: æ ‡å‡†åŒ–æ–¹æ³•å¯¹æ¯”")
        
        scalers = [
            ('StandardScaler', StandardScaler()),
            ('MinMaxScaler', MinMaxScaler()),
            ('RobustScaler', RobustScaler()),
            ('NoScaling', None)
        ]
        
        for name, scaler in scalers:
            if scaler is not None:
                X_scaled = scaler.fit_transform(self.X)
            else:
                X_scaled = self.X.copy()
            
            # t-SNEé™ç»´
            tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
            X_reduced = tsne.fit_transform(X_scaled)
            
            # Wardèšç±»
            ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
            cluster_labels = ward.fit_predict(X_reduced)
            
            silhouette = silhouette_score(X_reduced, cluster_labels)
            
            potential_scores = calculate_potential_score(self.df)
            district_labels_df = assign_district_labels(potential_scores)
            consistency, _, _ = evaluate_semantic_consistency(cluster_labels, district_labels_df, self.districts)
            
            self.results.append({
                'method': f'{name} + t-SNE + Ward',
                'silhouette': silhouette,
                'consistency': consistency,
                'n_clusters': len(np.unique(cluster_labels)),
                'details': f'æ ‡å‡†åŒ–: {name}'
            })
            
            print(f"  {name:15s}: Silhouette={silhouette:.3f}, ä¸€è‡´æ€§={consistency:.3f}")
    
    def experiment_5_ensemble_method(self):
        """å®éªŒ5: é›†æˆèšç±»æ–¹æ³•"""
        print("\nğŸ”¬ å®éªŒ5: é›†æˆèšç±»æ–¹æ³•")
        
        # å¤šç§æ–¹æ³•ç»„åˆ
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        
        # æ–¹æ³•1: PCA + KMeans
        pca = PCA(n_components=2, random_state=RANDOM_STATE)
        X_pca = pca.fit_transform(X_scaled)
        kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE)
        labels_1 = kmeans.fit_predict(X_pca)
        
        # æ–¹æ³•2: t-SNE + Ward
        tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
        X_tsne = tsne.fit_transform(X_scaled)
        ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
        labels_2 = ward.fit_predict(X_tsne)
        
        # é›†æˆæŠ•ç¥¨
        ensemble_labels = []
        for i in range(len(self.districts)):
            votes = [labels_1[i], labels_2[i]]
            # ç®€å•æŠ•ç¥¨æœºåˆ¶
            unique, counts = np.unique(votes, return_counts=True)
            ensemble_labels.append(unique[np.argmax(counts)])
        
        ensemble_labels = np.array(ensemble_labels)
        
        # å¦‚æœé›†ç¾¤æ•°é‡ä¸è¶³3ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•2
        if len(np.unique(ensemble_labels)) < 3:
            ensemble_labels = labels_2
        
        silhouette = silhouette_score(X_tsne, ensemble_labels)  # ä½¿ç”¨t-SNEç©ºé—´è®¡ç®—
        
        potential_scores = calculate_potential_score(self.df)
        district_labels_df = assign_district_labels(potential_scores)
        consistency, _, _ = evaluate_semantic_consistency(ensemble_labels, district_labels_df, self.districts)
        
        self.results.append({
            'method': 'Ensemble (PCA+KMeans & t-SNE+Ward)',
            'silhouette': silhouette,
            'consistency': consistency,
            'n_clusters': len(np.unique(ensemble_labels)),
            'details': 'é›†æˆæ–¹æ³•'
        })
        
        print(f"  é›†æˆæ–¹æ³•: Silhouette={silhouette:.3f}, ä¸€è‡´æ€§={consistency:.3f}")
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        print("ğŸš€ å¼€å§‹é«˜çº§èšç±»ä¼˜åŒ–å®éªŒ")
        print("ç›®æ ‡: Silhouette > 0.6 ä¸” ä¸€è‡´æ€§ > 0.7 ä¸” åˆ†ç¾¤æ•° >= 3")
        print("="*60)
        
        self.experiment_1_algorithm_comparison()
        self.experiment_2_dimensionality_reduction()
        self.experiment_3_feature_selection()
        self.experiment_4_scaling_methods()
        self.experiment_5_ensemble_method()
        
        return self.results
    
    def analyze_results(self):
        """åˆ†æç»“æœ"""
        print("\nğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("="*60)
        
        # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
        df_results = pd.DataFrame(self.results)
        
        # ç­›é€‰æ»¡è¶³æ¡ä»¶çš„ç»“æœ
        target_silhouette = 0.6
        target_consistency = 0.7
        target_clusters = 3
        
        print(f"ç­›é€‰æ¡ä»¶: Silhouette > {target_silhouette}, ä¸€è‡´æ€§ > {target_consistency}, åˆ†ç¾¤æ•° >= {target_clusters}")
        
        good_results = df_results[
            (df_results['silhouette'] > target_silhouette) & 
            (df_results['consistency'] > target_consistency) & 
            (df_results['n_clusters'] >= target_clusters)
        ]
        
        if len(good_results) > 0:
            print(f"\nâœ… æ‰¾åˆ° {len(good_results)} ä¸ªæ»¡è¶³æ¡ä»¶çš„æ–¹æ³•:")
            for _, row in good_results.iterrows():
                print(f"  {row['method']:30s}: Silhouette={row['silhouette']:.3f}, ä¸€è‡´æ€§={row['consistency']:.3f}")
        else:
            print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°å®Œå…¨æ»¡è¶³æ¡ä»¶çš„æ–¹æ³•")
            print(f"\nğŸ“ˆ æœ€ä½³ç»“æœ (æŒ‰ç»¼åˆåˆ†æ•°æ’åº):")
            
            # è®¡ç®—ç»¼åˆåˆ†æ•°
            df_results['combined_score'] = (
                0.4 * df_results['silhouette'] + 
                0.6 * df_results['consistency']
            )
            
            best_results = df_results.nlargest(5, 'combined_score')
            for _, row in best_results.iterrows():
                print(f"  {row['method']:30s}: Silhouette={row['silhouette']:.3f}, ä¸€è‡´æ€§={row['consistency']:.3f}, ç»¼åˆ={row['combined_score']:.3f}")
        
        return good_results if len(good_results) > 0 else best_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ é«˜çº§èšç±»ä¼˜åŒ–å®éªŒ")
    print("="*60)
    
    # è½½å…¥æ•°æ®
    X, districts, features, df = load_data()
    
    # è¿è¡Œå®éªŒ
    experiment = ClusteringExperiment(X, districts, df)
    results = experiment.run_all_experiments()
    
    # åˆ†æç»“æœ
    best_methods = experiment.analyze_results()
    
    print(f"\nâœ… å®éªŒå®Œæˆ! å…±æµ‹è¯•äº† {len(results)} ç§æ–¹æ³•")
    
    return best_methods

if __name__ == "__main__":
    best_methods = main() 