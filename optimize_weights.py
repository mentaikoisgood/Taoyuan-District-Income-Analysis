"""
æ¬Šé‡å„ªåŒ–åˆ†æ
æ‰¾åˆ°æ›´ç©©å®šçš„æ¬Šé‡é…ç½®ä»¥æ”¹å–„3ç´šJenksåˆ†ç´šçš„æ•æ„Ÿåº¦å•é¡Œ

åŸºæ–¼æ•æ„Ÿåº¦åˆ†æçµæœï¼Œæ¸¬è©¦ä¸åŒçš„æ¬Šé‡çµ„åˆ
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import json
import itertools
import warnings
warnings.filterwarnings('ignore')

try:
    import jenkspy
    JENKS_AVAILABLE = True
except ImportError:
    JENKS_AVAILABLE = False
    print("âŒ Jenksæœªå®‰è£")
    exit(1)

def load_data():
    """è¼‰å…¥æ•¸æ“š"""
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    districts = df['å€åŸŸåˆ¥'].tolist()
    feature_names = df.columns[1:].tolist()
    return df, districts, feature_names

def calculate_scores_and_ranking(df, feature_names, weights):
    """è¨ˆç®—åˆ†æ•¸å’Œæ’å"""
    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(df[feature_names])
    
    # è¨ˆç®—ç¶œåˆåˆ†æ•¸
    composite_scores = np.zeros(len(df))
    
    for i, feature in enumerate(feature_names):
        weight = weights[feature]['weight']
        direction = weights[feature]['direction']
        
        if direction == 'positive':
            feature_score = X_standardized[:, i] * weight
        else:
            feature_score = -X_standardized[:, i] * weight
        
        composite_scores += feature_score
    
    # æ­£è¦åŒ–åˆ°0-100åˆ†
    min_score = np.min(composite_scores)
    max_score = np.max(composite_scores)
    normalized_scores = ((composite_scores - min_score) / (max_score - min_score)) * 100
    
    # è¨ˆç®—æ’å
    ranking = len(normalized_scores) + 1 - pd.Series(normalized_scores).rank(method='min')
    
    return normalized_scores, ranking

def test_weight_stability(df, feature_names, weights, districts, n_tests=20):
    """æ¸¬è©¦æ¬Šé‡é…ç½®çš„ç©©å®šæ€§"""
    base_scores, base_ranking = calculate_scores_and_ranking(df, feature_names, weights)
    
    # ç”Ÿæˆæ¬Šé‡è®Šå‹•
    correlations = []
    top5_stabilities = []
    
    weight_delta = 0.05
    feature_list = list(weights.keys())
    
    for _ in range(n_tests):
        # éš¨æ©Ÿé¸æ“‡ä¸€å€‹ç‰¹å¾µé€²è¡Œè®Šå‹•
        target_feature = np.random.choice(feature_list)
        direction = np.random.choice([-1, 1])
        
        # å‰µå»ºè®Šå‹•å¾Œçš„æ¬Šé‡
        modified_weights = {}
        for feature, props in weights.items():
            modified_weights[feature] = props.copy()
        
        # èª¿æ•´æ¬Šé‡
        new_weight = weights[target_feature]['weight'] + (direction * weight_delta)
        if new_weight < 0:
            new_weight = 0.01
            
        modified_weights[target_feature]['weight'] = new_weight
        
        # é‡æ–°æ¨™æº–åŒ–
        total_weight = sum(props['weight'] for props in modified_weights.values())
        for feature in modified_weights:
            modified_weights[feature]['weight'] /= total_weight
        
        try:
            # è¨ˆç®—æ–°åˆ†æ•¸å’Œæ’å
            new_scores, new_ranking = calculate_scores_and_ranking(df, feature_names, modified_weights)
            
            # è¨ˆç®—ç›¸é—œæ€§
            corr = np.corrcoef(base_ranking, new_ranking)[0, 1]
            if not np.isnan(corr):
                correlations.append(corr)
            
            # Top 5ç©©å®šæ€§
            base_top5 = set(np.argsort(base_ranking)[:5])
            new_top5 = set(np.argsort(new_ranking)[:5])
            overlap = len(base_top5.intersection(new_top5)) / 5
            top5_stabilities.append(overlap)
            
        except:
            continue
    
    if len(correlations) == 0:
        return 0, 0
    
    avg_corr = np.mean(correlations)
    avg_top5 = np.mean(top5_stabilities)
    
    return avg_corr, avg_top5

def generate_weight_combinations():
    """ç”Ÿæˆä¸åŒçš„æ¬Šé‡çµ„åˆé€²è¡Œæ¸¬è©¦"""
    # ç‰¹å¾µåˆ—è¡¨
    features = ['äººå£_working_age_ratio', 'å•†æ¥­_hhi_index', 'æ‰€å¾—_median_household_income', 
               'tertiary_industry_ratio', 'medical_index']
    
    # æ¬Šé‡å€™é¸å€¼ (å¿…é ˆç¸½å’Œç‚º1)
    weight_configs = [
        # é…ç½®1: å‡ç­‰æ¬Šé‡
        {
            'äººå£_working_age_ratio': 0.20,
            'å•†æ¥­_hhi_index': 0.20, 
            'æ‰€å¾—_median_household_income': 0.20,
            'tertiary_industry_ratio': 0.20,
            'medical_index': 0.20
        },
        # é…ç½®2: æ‰€å¾—ä¸»å°
        {
            'äººå£_working_age_ratio': 0.15,
            'å•†æ¥­_hhi_index': 0.10,
            'æ‰€å¾—_median_household_income': 0.40,
            'tertiary_industry_ratio': 0.20,
            'medical_index': 0.15
        },
        # é…ç½®3: é†«ç™‚å’Œæ‰€å¾—ä¸¦é‡
        {
            'äººå£_working_age_ratio': 0.15,
            'å•†æ¥­_hhi_index': 0.10,
            'æ‰€å¾—_median_household_income': 0.30,
            'tertiary_industry_ratio': 0.15,
            'medical_index': 0.30
        },
        # é…ç½®4: ç”¢æ¥­ä¸»å°
        {
            'äººå£_working_age_ratio': 0.20,
            'å•†æ¥­_hhi_index': 0.15,
            'æ‰€å¾—_median_household_income': 0.20,
            'tertiary_industry_ratio': 0.35,
            'medical_index': 0.10
        },
        # é…ç½®5: äººå£å’Œæ‰€å¾—é‡è¦–
        {
            'äººå£_working_age_ratio': 0.30,
            'å•†æ¥­_hhi_index': 0.10,
            'æ‰€å¾—_median_household_income': 0.35,
            'tertiary_industry_ratio': 0.15,
            'medical_index': 0.10
        },
        # é…ç½®6: å¹³è¡¡å‹(é™ä½æ‰€å¾—æ¬Šé‡)
        {
            'äººå£_working_age_ratio': 0.22,
            'å•†æ¥­_hhi_index': 0.18,
            'æ‰€å¾—_median_household_income': 0.20,
            'tertiary_industry_ratio': 0.20,
            'medical_index': 0.20
        },
        # é…ç½®7: ä¿å®ˆå‹(é™ä½è®Šå‹•æœ€å¤§çš„ç‰¹å¾µæ¬Šé‡)
        {
            'äººå£_working_age_ratio': 0.15,  # é™ä½æœ€æ•æ„Ÿçš„ç‰¹å¾µ
            'å•†æ¥­_hhi_index': 0.20,
            'æ‰€å¾—_median_household_income': 0.25,
            'tertiary_industry_ratio': 0.20,
            'medical_index': 0.20
        }
    ]
    
    # è½‰æ›ç‚ºå®Œæ•´æ ¼å¼
    full_configs = []
    for i, config in enumerate(weight_configs):
        full_config = {}
        for feature, weight in config.items():
            full_config[feature] = {
                'weight': weight,
                'direction': 'negative' if feature == 'å•†æ¥­_hhi_index' else 'positive',
                'description': f"é…ç½®{i+1}"
            }
        full_configs.append(full_config)
    
    return full_configs

def evaluate_configuration(df, districts, feature_names, weights):
    """è©•ä¼°æ¬Šé‡é…ç½®"""
    # è¨ˆç®—åŸºæœ¬åˆ†æ•¸
    scores, ranking = calculate_scores_and_ranking(df, feature_names, weights)
    
    # ä½¿ç”¨Jenksåˆ†ç´š
    try:
        breaks = jenkspy.jenks_breaks(scores, n_classes=3)
        labels = []
        
        for score in scores:
            if score <= breaks[1]:
                labels.append('ä½æ½›åŠ›')
            elif score <= breaks[2]:
                labels.append('ä¸­æ½›åŠ›')
            else:
                labels.append('é«˜æ½›åŠ›')
    except:
        return None
    
    # è¨ˆç®—è³ªé‡æŒ‡æ¨™
    def calculate_f_statistic(scores, labels):
        unique_labels = np.unique(labels)
        overall_mean = np.mean(scores)
        
        between_var = 0
        within_var = 0
        
        for label in unique_labels:
            mask = np.array(labels) == label
            group_scores = scores[mask]
            group_mean = np.mean(group_scores)
            group_size = len(group_scores)
            
            between_var += (group_mean - overall_mean) ** 2 * group_size
            if group_size > 1:
                within_var += np.var(group_scores) * group_size
        
        between_var /= len(scores)
        within_var /= len(scores)
        
        return between_var / within_var if within_var > 0 else 0
    
    f_stat = calculate_f_statistic(scores, labels)
    
    # æ¸¬è©¦ç©©å®šæ€§
    avg_corr, avg_top5 = test_weight_stability(df, feature_names, weights, districts)
    
    # è¨ˆç®—ç´šåˆ¥åˆ†å¸ƒ
    level_counts = pd.Series(labels).value_counts()
    balance_score = 1 - np.std([level_counts.get(level, 0) for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']]) / np.mean([level_counts.get(level, 0) for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']])
    
    return {
        'f_statistic': f_stat,
        'ranking_correlation': avg_corr,
        'top5_stability': avg_top5,
        'balance_score': balance_score,
        'scores': scores.tolist(),
        'ranking': ranking.tolist(),
        'labels': labels,
        'breaks': breaks
    }

def main():
    """ä¸»å‡½æ•¸"""
    print("âš–ï¸ æ¬Šé‡å„ªåŒ–åˆ†æ")
    print("="*50)
    
    # è¼‰å…¥æ•¸æ“š
    df, districts, feature_names = load_data()
    
    # ç”Ÿæˆæ¬Šé‡é…ç½®
    weight_configs = generate_weight_combinations()
    
    print(f"ğŸ“Š æ¸¬è©¦ {len(weight_configs)} ç¨®æ¬Šé‡é…ç½®...")
    
    results = []
    
    for i, weights in enumerate(weight_configs):
        print(f"\nğŸ” æ¸¬è©¦é…ç½® {i+1}:")
        
        # é¡¯ç¤ºæ¬Šé‡
        for feature, props in weights.items():
            direction_symbol = "+" if props['direction'] == 'positive' else "-"
            print(f"  {feature}: {props['weight']:.3f} ({direction_symbol})")
        
        # è©•ä¼°é…ç½®
        result = evaluate_configuration(df, districts, feature_names, weights)
        
        if result:
            result['config_id'] = i + 1
            result['weights'] = weights
            results.append(result)
            
            print(f"  Fçµ±è¨ˆé‡: {result['f_statistic']:.3f}")
            print(f"  æ’åç©©å®šæ€§: {result['ranking_correlation']:.3f}")
            print(f"  å‰5åç©©å®šæ€§: {result['top5_stability']:.3f}")
            print(f"  ç´šåˆ¥å¹³è¡¡æ€§: {result['balance_score']:.3f}")
        else:
            print("  âŒ è©•ä¼°å¤±æ•—")
    
    if not results:
        print("âŒ æ²’æœ‰æˆåŠŸçš„é…ç½®")
        return
    
    # è¨ˆç®—ç¶œåˆè©•åˆ†
    for result in results:
        # ç¶œåˆè©•åˆ† = Fçµ±è¨ˆé‡ * 0.3 + æ’åç©©å®šæ€§ * 0.4 + å‰5åç©©å®šæ€§ * 0.2 + å¹³è¡¡æ€§ * 0.1
        result['composite_score'] = (
            result['f_statistic'] * 0.3 +
            max(0, result['ranking_correlation']) * 0.4 +
            result['top5_stability'] * 0.2 +
            result['balance_score'] * 0.1
        )
    
    # æ’åºçµæœ
    results.sort(key=lambda x: x['composite_score'], reverse=True)
    
    print(f"\nğŸ† æ¬Šé‡é…ç½®æ’å:")
    print("="*50)
    
    for i, result in enumerate(results):
        print(f"{i+1}. é…ç½®{result['config_id']} - ç¶œåˆè©•åˆ†: {result['composite_score']:.3f}")
        print(f"   Fçµ±è¨ˆé‡: {result['f_statistic']:.3f} | æ’åç©©å®šæ€§: {result['ranking_correlation']:.3f}")
        print(f"   å‰5åç©©å®šæ€§: {result['top5_stability']:.3f} | å¹³è¡¡æ€§: {result['balance_score']:.3f}")
    
    # æ¨è–¦æœ€ä½³é…ç½®
    best_config = results[0]
    print(f"\nâœ… æ¨è–¦é…ç½®: é…ç½®{best_config['config_id']}")
    print("æ¨è–¦æ¬Šé‡:")
    for feature, props in best_config['weights'].items():
        direction_symbol = "+" if props['direction'] == 'positive' else "-"
        print(f"  '{feature}': {props['weight']:.3f} ({direction_symbol})")
    
    # ä¿å­˜çµæœ
    output_path = 'output/weight_optimization_results.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å„ªåŒ–çµæœå·²ä¿å­˜: {output_path}")
    
    return best_config

if __name__ == "__main__":
    best_config = main() 