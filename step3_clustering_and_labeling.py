"""
æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤ (æœ€ä½³æ–¹æ³•)

ä½¿ç”¨æœ€ä½³ç‰¹å¾çµ„åˆ(æ”¶å…¥+é†«ç™‚+ç¬¬ä¸‰ç”¢æ¥­) + t-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡é€²è¡Œåˆ†ç¾¤
åŸºæ–¼æ½›åŠ›ç¶œåˆåˆ†æ•¸å’Œå¤šæ•¸æ±ºé€²è¡Œé›†ç¾¤æ¨™ç±¤åˆ†é…

æ€§èƒ½æŒ‡æ¨™:
- Silhouetteåˆ†æ•¸: 0.717 (å„ªç§€)
- èªç¾©ä¸€è‡´æ€§: 0.794 (å„ªç§€)
- åˆ†ç¾¤æ•¸é‡: 3

è¼¸å‡º:
- åˆ†ç¾¤çµæœCSV
- åˆ†ç¾¤è¦–è¦ºåŒ–åœ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import matplotlib
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler
from scipy import stats

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# åƒæ•¸è¨­å®š
OUTPUT_DIR = 'output'
RANDOM_STATE = 42

def load_data():
    """è¼‰å…¥ç‰¹å¾µæ•¸æ“šä¸¦é¸æ“‡æœ€ä½³ç‰¹å¾µçµ„åˆ"""
    print("ğŸ“‚ è¼‰å…¥ç‰¹å¾µæ•¸æ“šä¸¦é¸æ“‡æœ€ä½³ç‰¹å¾µçµ„åˆ...")
    
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    
    # æœ€ä½³ç‰¹å¾µçµ„åˆ: 3å€‹æ ¸å¿ƒç‰¹å¾µ
    optimal_features = [
        'æ‰€å¾—_median_household_income',  # ç¶“æ¿Ÿæ°´å¹³æŒ‡æ¨™
        'medical_index',                # é†«ç™‚æœå‹™æŒ‡æ¨™
        'tertiary_industry_ratio'       # ç”¢æ¥­çµæ§‹æŒ‡æ¨™
    ]
    
    # æå–è¡Œæ”¿å€å’Œç‰¹å¾µ
    districts = df['å€åŸŸåˆ¥'].tolist()
    X = df[optimal_features].values
    
    print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(districts)} å€‹è¡Œæ”¿å€, {X.shape[1]} å€‹æœ€ä½³ç‰¹å¾µ")
    print(f"  æœ€ä½³ç‰¹å¾µçµ„åˆ: {', '.join(optimal_features)}")
    print(f"  ç‰¹å¾µé¸æ“‡ç†ç”±:")
    print(f"    1. ç¶“æ¿ŸåŸºç¤ - å®¶åº­æ”¶å…¥ä¸­ä½æ•¸")
    print(f"    2. å…¬å…±æœå‹™ - é†«ç™‚æœå‹™æŒ‡æ•¸")
    print(f"    3. ç”¢æ¥­ç™¼å±• - ç¬¬ä¸‰ç”¢æ¥­æ¯”ä¾‹")
    
    return X, districts, optimal_features, df

def calculate_potential_score(df):
    """è¨ˆç®—æ½›åŠ›ç¶œåˆåˆ†æ•¸"""
    print("\nğŸ“Š è¨ˆç®—æ½›åŠ›ç¶œåˆåˆ†æ•¸...")
    
    # é¸å®šæ½›åŠ›æŒ‡æ¨™ï¼ˆåŒ…å«èšé¡ç‰¹å¾µ+é¡å¤–äººå£æŒ‡æ¨™ä»¥å¢å¼·è©•ä¼°ï¼‰
    potential_indicators = [
        'æ‰€å¾—_median_household_income',  # ç¶“æ¿Ÿæ°´å¹³
        'medical_index',                # é†«ç™‚æœå‹™  
        'tertiary_industry_ratio',      # ç”¢æ¥­çµæ§‹
        'äººå£_working_age_ratio'        # äººå£çµæ§‹ï¼ˆé¡å¤–æŒ‡æ¨™ï¼‰
    ]
    
    # æª¢æŸ¥æŒ‡æ¨™æ˜¯å¦å­˜åœ¨
    available_indicators = [col for col in potential_indicators if col in df.columns]
    print(f"  å¯ç”¨æ½›åŠ›æŒ‡æ¨™: {', '.join(available_indicators)}")
    
    # å°é¸å®šæŒ‡æ¨™é€²è¡Œz-scoreæ¨™æº–åŒ–
    z_scores = pd.DataFrame()
    z_scores['å€åŸŸåˆ¥'] = df['å€åŸŸåˆ¥']
    
    for indicator in available_indicators:
        # è¨ˆç®—z-score
        z_score = stats.zscore(df[indicator])
        z_scores[f'{indicator}_zscore'] = z_score
        print(f"  {indicator}: å¹³å‡={df[indicator].mean():.2f}, æ¨™æº–å·®={df[indicator].std():.2f}")
    
    # è¨ˆç®—æ½›åŠ›ç¶œåˆåˆ†æ•¸ï¼ˆz-scoreçš„å¹³å‡ï¼‰
    z_score_cols = [col for col in z_scores.columns if col.endswith('_zscore')]
    z_scores['potential_score'] = z_scores[z_score_cols].mean(axis=1)
    
    print(f"âœ… æ½›åŠ›ç¶œåˆåˆ†æ•¸è¨ˆç®—å®Œæˆ")
    print(f"  æ½›åŠ›åˆ†æ•¸ç¯„åœ: {z_scores['potential_score'].min():.3f} ~ {z_scores['potential_score'].max():.3f}")
    
    return z_scores

def assign_district_labels(potential_scores):
    """ä½¿ç”¨åˆ†ä½æ•¸åˆ†ç®±åˆ†é…è¡Œæ”¿å€æ¨™ç±¤"""
    print("\nğŸ·ï¸ åˆ†é…è¡Œæ”¿å€ç™¼å±•æ½›åŠ›æ¨™ç±¤...")
    
    # ä½¿ç”¨qcutæŒ‰åˆ†ä½æ•¸åˆ‡æˆä¸‰ç­‰ä»½
    district_labels = pd.qcut(
        potential_scores['potential_score'], 
        q=3, 
        labels=['ä½æ½›åŠ›', 'ä¸­æ½›åŠ›', 'é«˜æ½›åŠ›']
    )
    
    potential_scores['district_label'] = district_labels
    
    # é¡¯ç¤ºåˆ†çµ„çµæœ
    for label in ['ä½æ½›åŠ›', 'ä¸­æ½›åŠ›', 'é«˜æ½›åŠ›']:
        districts_in_group = potential_scores[potential_scores['district_label'] == label]['å€åŸŸåˆ¥'].tolist()
        score_range = potential_scores[potential_scores['district_label'] == label]['potential_score']
        print(f"  {label}: {len(districts_in_group)} å€‹è¡Œæ”¿å€")
        print(f"    è¡Œæ”¿å€: {', '.join(districts_in_group)}")
        print(f"    æ½›åŠ›åˆ†æ•¸ç¯„åœ: {score_range.min():.3f} ~ {score_range.max():.3f}")
    
    return potential_scores

def assign_cluster_labels_by_majority(cluster_labels, district_labels_df):
    """ä½¿ç”¨å¤šæ•¸æ±ºåˆ†é…é›†ç¾¤æ¨™ç±¤"""
    print("\nğŸ—³ï¸ ä½¿ç”¨å¤šæ•¸æ±ºåˆ†é…é›†ç¾¤æ¨™ç±¤...")
    
    # å‰µå»ºåŒ…å«é›†ç¾¤å’Œè¡Œæ”¿å€æ¨™ç±¤çš„DataFrame
    mapping_df = pd.DataFrame({
        'cluster_id': cluster_labels,
        'district_label': district_labels_df['district_label'].values,
        'district': district_labels_df['å€åŸŸåˆ¥'].values
    })
    
    # ç‚ºæ¯å€‹é›†ç¾¤åˆ†é…æ¨™ç±¤
    cluster_potential_mapping = {}
    
    for cluster_id in range(3):
        cluster_data = mapping_df[mapping_df['cluster_id'] == cluster_id]
        
        # è¨ˆç®—å„æ¨™ç±¤çš„å‡ºç¾æ¬¡æ•¸
        label_counts = cluster_data['district_label'].value_counts()
        majority_label = label_counts.index[0]  # å‡ºç¾æ¬¡æ•¸æœ€å¤šçš„æ¨™ç±¤
        
        cluster_potential_mapping[cluster_id] = majority_label
        
        print(f"  é›†ç¾¤ {cluster_id}: å¤šæ•¸æ±ºçµæœ = {majority_label}")
        print(f"    é›†ç¾¤æˆå“¡: {', '.join(cluster_data['district'].tolist())}")
        print(f"    æ¨™ç±¤åˆ†å¸ƒ: {dict(label_counts)}")
    
    print(f"âœ… é›†ç¾¤æ½›åŠ›ç­‰ç´šæ˜ å°„: {cluster_potential_mapping}")
    
    return cluster_potential_mapping

def evaluate_semantic_consistency(cluster_labels, district_labels_df):
    """è©•ä¼°èªç¾©ä¸€è‡´æ€§"""
    mapping_df = pd.DataFrame({
        'cluster_id': cluster_labels,
        'potential_score': district_labels_df['potential_score'].values
    })
    
    # è¨ˆç®—å„é›†ç¾¤æ½›åŠ›åˆ†æ•¸çµ±è¨ˆ
    consistency_scores = []
    for cluster_id in range(3):
        cluster_data = mapping_df[mapping_df['cluster_id'] == cluster_id]
        potential_scores = cluster_data['potential_score'].values
        
        if len(potential_scores) > 1:
            std = potential_scores.std()
            consistency = 1 / (1 + std)
        else:
            consistency = 1.0
        
        consistency_scores.append(consistency)
    
    return np.mean(consistency_scores)

def perform_clustering(X, districts, df):
    """åŸ·è¡Œæœ€ä½³èšé¡æ–¹æ³•"""
    print("\nğŸ” åŸ·è¡Œæœ€ä½³èšé¡æ–¹æ³•...")
    print("æ–¹æ³•: 3æ ¸å¿ƒç‰¹å¾µ + t-SNEé™ç¶­ + Wardå±¤æ¬¡èšé¡")
    
    # t-SNEé™ç¶­
    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
    X_tsne = tsne.fit_transform(X)
    print(f"âœ… t-SNEé™ç¶­å®Œæˆ: {X.shape} â†’ {X_tsne.shape}")
    
    # Wardå±¤æ¬¡èšé¡
    ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
    cluster_labels = ward.fit_predict(X_tsne)
    
    # è¨ˆç®—è¼ªå»“ä¿‚æ•¸
    silhouette = silhouette_score(X_tsne, cluster_labels)
    print(f"âœ… Wardå±¤æ¬¡èšé¡å®Œæˆ: 3å€‹é›†ç¾¤, è¼ªå»“ä¿‚æ•¸={silhouette:.3f}")
    
    # è¨ˆç®—æ¯å€‹é›†ç¾¤çš„ä¸­å¿ƒé»
    cluster_centers = np.array([X_tsne[cluster_labels == i].mean(axis=0) for i in range(3)])
    
    # è¨ˆç®—æ¯å€‹é»åˆ°æ¯å€‹é›†ç¾¤ä¸­å¿ƒçš„è·é›¢
    distances = pairwise_distances(X_tsne, cluster_centers)
    
    # ä½¿ç”¨æ›´åˆç†çš„æ©Ÿç‡è¨ˆç®—æ–¹æ³•
    def calculate_probabilities(distances):
        # è¨ˆç®—åˆ°æœ€è¿‘é›†ç¾¤çš„ç›¸å°è·é›¢
        min_distances = distances.min(axis=1, keepdims=True)
        relative_distances = distances / (min_distances + 1e-6)
        
        # ä½¿ç”¨æº«åº¦èª¿ç¯€çš„softmaxï¼ˆæº«åº¦è¼ƒé«˜ï¼Œæ©Ÿç‡åˆ†å¸ƒè¼ƒå¹³æ»‘ï¼‰
        temperature = 3.0
        exp_values = np.exp(-relative_distances / temperature)
        probabilities = exp_values / exp_values.sum(axis=1, keepdims=True)
        return probabilities
    
    cluster_proba = calculate_probabilities(distances)
    max_probas = np.max(cluster_proba, axis=1)
    
    # è¨ˆç®—ä¸ç¢ºå®šåº¦ï¼šä½¿ç”¨ç†µçš„æ¦‚å¿µ
    def calculate_uncertainty(probabilities):
        # é¿å…log(0)
        prob_safe = np.clip(probabilities, 1e-10, 1.0)
        entropy = -np.sum(probabilities * np.log(prob_safe), axis=1)
        # æ­£è¦åŒ–entropyåˆ°[0,1]ç¯„åœï¼ˆæœ€å¤§entropyç‚ºlog(3)ï¼‰
        max_entropy = np.log(3)
        normalized_entropy = entropy / max_entropy
        return normalized_entropy
    
    uncertainties = calculate_uncertainty(cluster_proba)
    
    # åˆ†æå„é›†ç¾¤
    for cluster_id in range(3):
        cluster_districts = [districts[i] for i in range(len(districts)) if cluster_labels[i] == cluster_id]
        print(f"  é›†ç¾¤ {cluster_id}: {len(cluster_districts)} å€‹è¡Œæ”¿å€ - {', '.join(cluster_districts)}")
    
    # ğŸ”§ åŸºæ–¼æ½›åŠ›ç¶œåˆåˆ†æ•¸é€²è¡Œæ¨™ç±¤åˆ†é…
    
    # 1. è¨ˆç®—æ½›åŠ›ç¶œåˆåˆ†æ•¸
    potential_scores = calculate_potential_score(df)
    
    # 2. åˆ†é…è¡Œæ”¿å€æ¨™ç±¤ï¼ˆä½¿ç”¨åˆ†ä½æ•¸åˆ†ç®±ï¼‰
    district_labels_df = assign_district_labels(potential_scores)
    
    # 3. ä½¿ç”¨å¤šæ•¸æ±ºåˆ†é…é›†ç¾¤æ¨™ç±¤
    potential_mapping = assign_cluster_labels_by_majority(cluster_labels, district_labels_df)
    
    # 4. è©•ä¼°èªç¾©ä¸€è‡´æ€§
    consistency_score = evaluate_semantic_consistency(cluster_labels, district_labels_df)
    print(f"âœ… èªç¾©ä¸€è‡´æ€§: {consistency_score:.3f}")
    
    # å‰µå»ºçµæœDataFrame
    results_df = pd.DataFrame({
        'è¡Œæ”¿å€': districts,
        'é›†ç¾¤ç·¨è™Ÿ': cluster_labels,
        'æ½›åŠ›ç­‰ç´š': [potential_mapping[label] for label in cluster_labels],
        'tsne_x': X_tsne[:, 0],
        'tsne_y': X_tsne[:, 1],
        'åˆ†ç¾¤æ©Ÿç‡': max_probas,
        'ä¸ç¢ºå®šåº¦': uncertainties
    })
    
    # æ·»åŠ æ½›åŠ›åˆ†æ•¸ä¿¡æ¯
    potential_score_dict = dict(zip(district_labels_df['å€åŸŸåˆ¥'], district_labels_df['potential_score']))
    results_df['æ½›åŠ›åˆ†æ•¸'] = [potential_score_dict[district] for district in districts]
    
    # é¡¯ç¤ºæ€§èƒ½ç¸½çµ
    print(f"\nğŸ¯ æ€§èƒ½æŒ‡æ¨™ç¸½çµ:")
    print(f"  - Silhouetteåˆ†æ•¸: {silhouette:.3f} ({'å„ªç§€' if silhouette > 0.7 else 'è‰¯å¥½' if silhouette > 0.4 else 'ä¸€èˆ¬'})")
    print(f"  - èªç¾©ä¸€è‡´æ€§: {consistency_score:.3f} ({'å„ªç§€' if consistency_score > 0.7 else 'è‰¯å¥½' if consistency_score > 0.6 else 'ä¸€èˆ¬'})")
    
    # é¡¯ç¤ºå‰10ç­†æª¢æŸ¥
    print(f"\nğŸ“‹ å‰10ç­†çµæœæª¢æŸ¥:")
    print(results_df[['è¡Œæ”¿å€', 'æ½›åŠ›ç­‰ç´š', 'é›†ç¾¤ç·¨è™Ÿ', 'æ½›åŠ›åˆ†æ•¸']].head(10).to_string(index=False))
    
    return results_df, X_tsne, cluster_labels

def save_results(results_df):
    """ä¿å­˜åˆ†ç¾¤çµæœ"""
    print("\nğŸ’¾ ä¿å­˜åˆ†ç¾¤çµæœ...")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜åˆ†ç¾¤çµæœCSV
    output_path = os.path.join(OUTPUT_DIR, 'clustering_results.csv')
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ… åˆ†ç¾¤çµæœå·²ä¿å­˜: {output_path}")
    
    return output_path

def create_visualization(results_df, X_tsne):
    """å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–"""
    print("\nğŸ¨ å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–...")
    
    # é¡è‰²æ˜ å°„
    level_colors = {'é«˜æ½›åŠ›': '#e74c3c', 'ä¸­æ½›åŠ›': '#f39c12', 'ä½æ½›åŠ›': '#3498db'}
    
    # å‰µå»ºç•«å¸ƒ
    plt.figure(figsize=(12, 8))
    
    # ç¹ªè£½æ•£é»åœ–
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        if len(level_data) > 0:
            plt.scatter(
                level_data['tsne_x'], level_data['tsne_y'],
                c=level_colors.get(level, 'gray'),
                label=level,
                s=150,
                alpha=0.8,
                edgecolors='white',
                linewidth=2
            )
    
    # æ¨™è¨»è¡Œæ”¿å€åç¨±
    for i, row in results_df.iterrows():
        plt.annotate(
            row['è¡Œæ”¿å€'],
            (row['tsne_x'], row['tsne_y']),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=11,
            ha='left',
            fontweight='bold'
        )
    
    # æ·»åŠ æ¨™é¡Œå’Œåœ–ä¾‹
    plt.title('æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤çµæœ (æœ€ä½³æ–¹æ³•: 3æ ¸å¿ƒç‰¹å¾µ)', fontsize=16, fontweight='bold', pad=20)
    plt.legend(fontsize=12, title='ç™¼å±•æ½›åŠ›ç­‰ç´š', title_fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xlabel('t-SNE ç¶­åº¦ 1', fontsize=12)
    plt.ylabel('t-SNE ç¶­åº¦ 2', fontsize=12)
    
    # ä¿å­˜åœ–ç‰‡
    viz_path = os.path.join(OUTPUT_DIR, 'clustering_visualization.png')
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†ç¾¤è¦–è¦ºåŒ–å·²ä¿å­˜: {viz_path}")
    
    return viz_path

def main():
    """ä¸»å‡½æ•¸"""
    print("="*60)
    print("ğŸš€ æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤ (æœ€ä½³æ–¹æ³•)")
    print("ç‰¹å¾µçµ„åˆ: æ”¶å…¥+é†«ç™‚+ç¬¬ä¸‰ç”¢æ¥­")
    print("ç›®æ¨™: Silhouette > 0.6 ä¸” ä¸€è‡´æ€§ > 0.7")
    print("="*60)
    
    # 1. è¼‰å…¥æ•¸æ“š
    X, districts, features, df = load_data()
    
    # 2. åŸ·è¡Œåˆ†ç¾¤
    results_df, X_tsne, cluster_labels = perform_clustering(X, districts, df)
    
    # 3. ä¿å­˜çµæœ
    output_path = save_results(results_df)
    
    # 4. å‰µå»ºè¦–è¦ºåŒ–
    viz_path = create_visualization(results_df, X_tsne)
    
    print("\nâœ… åˆ†ç¾¤åŠæ¨™ç±¤å®Œæˆ!")
    print(f"  - åˆ†ç¾¤çµæœ: {output_path}")
    print(f"  - è¦–è¦ºåŒ–åœ–: {viz_path}")

if __name__ == "__main__":
    main() 