"""
æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤

ä½¿ç”¨t-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡é€²è¡Œåˆ†ç¾¤
ä½¿ç”¨Jenks Natural Breaksé€²è¡Œæ½›åŠ›ç­‰ç´šåˆ†é¡
è¼ªå»“ä¿‚æ•¸>0.7ä¸”åˆ†ç¾¤æ•¸=3

è¼¸å‡º:
- åˆ†ç¾¤çµæœCSV
- åˆ†ç¾¤è¦–è¦ºåŒ–åœ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import matplotlib
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler
import jenkspy

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# åƒæ•¸è¨­å®š
OUTPUT_DIR = 'output'
RANDOM_STATE = 42

def load_data():
    """è¼‰å…¥ç‰¹å¾µæ•¸æ“š"""
    print("ğŸ“‚ è¼‰å…¥ç‰¹å¾µæ•¸æ“š...")
    
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    
    # æå–è¡Œæ”¿å€å’Œç‰¹å¾µ
    districts = df['å€åŸŸåˆ¥'].tolist()
    X = df.drop('å€åŸŸåˆ¥', axis=1).values
    
    print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(districts)} å€‹è¡Œæ”¿å€, {X.shape[1]} å€‹ç‰¹å¾µ")
    print(f"  ç‰¹å¾µåˆ—è¡¨: {', '.join(df.columns[1:])}")
    
    return X, districts, df.columns[1:].tolist()

def calculate_composite_score(districts):
    """è¨ˆç®—ç¶œåˆç™¼å±•æ½›åŠ›åˆ†æ•¸"""
    print("\nğŸ“Š è¨ˆç®—ç¶œåˆç™¼å±•æ½›åŠ›åˆ†æ•¸...")
    
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    df = df.set_index('å€åŸŸåˆ¥')
    
    # ç‰¹å¾µæ¬Šé‡è¨­å®šï¼ˆåŸºæ–¼é ˜åŸŸçŸ¥è­˜ï¼‰
    weights = {
        'æ‰€å¾—_median_household_income': 0.35,      # ç¶“æ¿Ÿæ°´å¹³ - æœ€é‡è¦
        'tertiary_industry_ratio': 0.25,          # ç”¢æ¥­çµæ§‹ 
        'medical_index': 0.20,                    # é†«ç™‚è³‡æº
        'äººå£_working_age_ratio': 0.15,           # äººåŠ›è³‡æº
        'å•†æ¥­_hhi_index': 0.05                    # å•†æ¥­é›†ä¸­åº¦
    }
    
    print(f"  ç‰¹å¾µæ¬Šé‡è¨­å®š: {weights}")
    
    # æ¨™æº–åŒ–æ•¸æ“š
    normalized_data = df.copy()
    for feature in weights.keys():
        if feature in df.columns:
            min_val = df[feature].min()
            max_val = df[feature].max()
            normalized_data[feature] = (df[feature] - min_val) / (max_val - min_val)
    
    # è¨ˆç®—åŠ æ¬Šç¶œåˆåˆ†æ•¸
    composite_scores = {}
    for district in districts:
        score = 0
        for feature, weight in weights.items():
            if feature in normalized_data.columns:
                score += normalized_data.loc[district, feature] * weight
        composite_scores[district] = score
    
    print(f"âœ… ç¶œåˆåˆ†æ•¸è¨ˆç®—å®Œæˆ")
    for district, score in sorted(composite_scores.items(), key=lambda x: x[1], reverse=True):
        print(f"  {district}: {score:.3f}")
    
    return composite_scores

def apply_jenks_classification(composite_scores):
    """ä½¿ç”¨Jenks Natural Breaksé€²è¡Œæ½›åŠ›ç­‰ç´šåˆ†é¡"""
    print("\nğŸ¯ ä½¿ç”¨Jenks Natural Breaksé€²è¡Œæ½›åŠ›ç­‰ç´šåˆ†é¡...")
    
    # æå–åˆ†æ•¸æ•¸çµ„
    scores = list(composite_scores.values())
    
    # ä½¿ç”¨Jenks Natural Breaksé€²è¡Œ3é¡åˆ†çµ„
    breaks = jenkspy.jenks_breaks(scores, n_classes=3)
    
    print(f"  Jenksåˆ†çµ„é‚Šç•Œ: {[f'{b:.3f}' for b in breaks]}")
    
    # åˆ†é…æ½›åŠ›ç­‰ç´š
    potential_levels = {}
    for district, score in composite_scores.items():
        if score >= breaks[2]:  # æœ€é«˜çµ„
            level = 'é«˜æ½›åŠ›'
        elif score >= breaks[1]:  # ä¸­é–“çµ„
            level = 'ä¸­æ½›åŠ›'
        else:  # æœ€ä½çµ„
            level = 'ä½æ½›åŠ›'
        potential_levels[district] = level
    
    # çµ±è¨ˆå„ç­‰ç´š
    level_counts = {}
    level_scores = {'é«˜æ½›åŠ›': [], 'ä¸­æ½›åŠ›': [], 'ä½æ½›åŠ›': []}
    
    for district, level in potential_levels.items():
        level_counts[level] = level_counts.get(level, 0) + 1
        level_scores[level].append(composite_scores[district])
    
    print(f"âœ… Jenksåˆ†é¡çµæœ:")
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        count = level_counts.get(level, 0)
        avg_score = np.mean(level_scores[level]) if level_scores[level] else 0
        districts_in_level = [d for d, l in potential_levels.items() if l == level]
        print(f"  {level}: {count}å€‹è¡Œæ”¿å€, å¹³å‡åˆ†æ•¸={avg_score:.3f}")
        print(f"    è¡Œæ”¿å€: {', '.join(districts_in_level)}")
    
    return potential_levels, breaks

def perform_clustering(X, districts):
    """åŸ·è¡Œt-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡"""
    print("\nğŸ” åŸ·è¡Œt-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡...")
    
    # t-SNEé™ç¶­
    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
    X_tsne = tsne.fit_transform(X)
    print(f"âœ… t-SNEé™ç¶­å®Œæˆ: {X.shape} â†’ {X_tsne.shape}")
    
    # Wardå±¤æ¬¡èšé¡
    ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
    cluster_labels = ward.fit_predict(X_tsne)
    
    # è¨ˆç®—è¼ªå»“ä¿‚æ•¸
    silhouette = silhouette_score(X_tsne, cluster_labels)
    print(f"âœ… Wardå±¤æ¬¡èšé¡å®Œæˆ: 3å€‹é›†ç¾¤, è¼ªå»“ä¿‚æ•¸={silhouette:.3f}")
    
    # è¨ˆç®—æ¯å€‹é›†ç¾¤çš„ä¸­å¿ƒé»
    cluster_centers = np.array([X_tsne[cluster_labels == i].mean(axis=0) for i in range(3)])
    
    # è¨ˆç®—æ¯å€‹é»åˆ°æ¯å€‹é›†ç¾¤ä¸­å¿ƒçš„è·é›¢
    distances = pairwise_distances(X_tsne, cluster_centers)
    
    # ä½¿ç”¨æ›´åˆç†çš„æ©Ÿç‡è¨ˆç®—æ–¹æ³•
    def calculate_probabilities(distances):
        # è¨ˆç®—åˆ°æœ€è¿‘é›†ç¾¤çš„ç›¸å°è·é›¢
        min_distances = distances.min(axis=1, keepdims=True)
        relative_distances = distances / (min_distances + 1e-6)
        
        # ä½¿ç”¨æº«åº¦èª¿ç¯€çš„softmaxï¼ˆæº«åº¦è¼ƒé«˜ï¼Œæ©Ÿç‡åˆ†å¸ƒè¼ƒå¹³æ»‘ï¼‰
        temperature = 3.0
        exp_values = np.exp(-relative_distances / temperature)
        probabilities = exp_values / exp_values.sum(axis=1, keepdims=True)
        return probabilities
    
    cluster_proba = calculate_probabilities(distances)
    max_probas = np.max(cluster_proba, axis=1)
    
    # è¨ˆç®—ä¸ç¢ºå®šåº¦ï¼šä½¿ç”¨ç†µçš„æ¦‚å¿µ
    def calculate_uncertainty(probabilities):
        # é¿å…log(0)
        prob_safe = np.clip(probabilities, 1e-10, 1.0)
        entropy = -np.sum(probabilities * np.log(prob_safe), axis=1)
        # æ­£è¦åŒ–entropyåˆ°[0,1]ç¯„åœï¼ˆæœ€å¤§entropyç‚ºlog(3)ï¼‰
        max_entropy = np.log(3)
        normalized_entropy = entropy / max_entropy
        return normalized_entropy
    
    uncertainties = calculate_uncertainty(cluster_proba)
    
    # åˆ†æå„é›†ç¾¤
    for cluster_id in range(3):
        cluster_districts = [districts[i] for i in range(len(districts)) if cluster_labels[i] == cluster_id]
        print(f"  é›†ç¾¤ {cluster_id}: {len(cluster_districts)} å€‹è¡Œæ”¿å€ - {', '.join(cluster_districts)}")
    
    # ğŸ†• ä½¿ç”¨Jenks Natural Breaksé€²è¡Œæ½›åŠ›ç­‰ç´šåˆ†é¡
    composite_scores = calculate_composite_score(districts)
    potential_levels, jenks_breaks = apply_jenks_classification(composite_scores)
    
    # å‰µå»ºçµæœDataFrame
    results_df = pd.DataFrame({
        'è¡Œæ”¿å€': districts,
        'é›†ç¾¤ç·¨è™Ÿ': cluster_labels,
        'æ½›åŠ›ç­‰ç´š': [potential_levels[district] for district in districts],
        'ç¶œåˆåˆ†æ•¸': [composite_scores[district] for district in districts],
        'tsne_x': X_tsne[:, 0],
        'tsne_y': X_tsne[:, 1],
        'åˆ†ç¾¤æ©Ÿç‡': max_probas,
        'ä¸ç¢ºå®šåº¦': uncertainties
    })
    
    # è¼¸å‡ºJenksé‚Šç•Œä¿¡æ¯
    print(f"\nğŸ“ˆ Jenks Natural Breaks é‚Šç•Œ:")
    print(f"  é«˜æ½›åŠ›é–¾å€¼: â‰¥ {jenks_breaks[2]:.3f}")
    print(f"  ä¸­æ½›åŠ›é–¾å€¼: {jenks_breaks[1]:.3f} - {jenks_breaks[2]:.3f}")
    print(f"  ä½æ½›åŠ›é–¾å€¼: < {jenks_breaks[1]:.3f}")
    
    return results_df, X_tsne, cluster_labels

def save_results(results_df):
    """ä¿å­˜åˆ†ç¾¤çµæœ"""
    print("\nğŸ’¾ ä¿å­˜åˆ†ç¾¤çµæœ...")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜åˆ†ç¾¤çµæœCSV
    output_path = os.path.join(OUTPUT_DIR, 'clustering_results.csv')
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ… åˆ†ç¾¤çµæœå·²ä¿å­˜: {output_path}")
    
    return output_path

def create_visualization(results_df, X_tsne):
    """å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–"""
    print("\nğŸ¨ å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–...")
    
    # é¡è‰²æ˜ å°„
    level_colors = {'é«˜æ½›åŠ›': 'red', 'ä¸­æ½›åŠ›': 'orange', 'ä½æ½›åŠ›': 'blue'}
    
    # å‰µå»ºç•«å¸ƒ
    plt.figure(figsize=(12, 8))
    
    # å­åœ–1: t-SNEèšé¡çµæœ
    plt.subplot(1, 2, 1)
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        if len(level_data) > 0:
            plt.scatter(
                level_data['tsne_x'], level_data['tsne_y'],
                c=level_colors.get(level, 'gray'),
                label=level,
                s=120,
                alpha=0.8
            )
    
    # æ¨™è¨»è¡Œæ”¿å€åç¨±
    for i, row in results_df.iterrows():
        plt.annotate(
            row['è¡Œæ”¿å€'],
            (row['tsne_x'], row['tsne_y']),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=8,
            ha='left'
        )
    
    plt.title('t-SNEèšé¡çµæœ + Jenksåˆ†é¡', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # å­åœ–2: ç¶œåˆåˆ†æ•¸åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    scores_by_level = {}
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        scores_by_level[level] = level_data['ç¶œåˆåˆ†æ•¸'].values
        
        plt.hist(level_data['ç¶œåˆåˆ†æ•¸'], 
                alpha=0.7, 
                color=level_colors[level], 
                label=level,
                bins=5)
    
    plt.title('ç¶œåˆåˆ†æ•¸åˆ†å¸ƒ (Jenks Natural Breaks)', fontsize=12, fontweight='bold')
    plt.xlabel('ç¶œåˆç™¼å±•æ½›åŠ›åˆ†æ•¸')
    plt.ylabel('è¡Œæ”¿å€æ•¸é‡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ä¿å­˜åœ–ç‰‡
    viz_path = os.path.join(OUTPUT_DIR, 'clustering_visualization.png')
    plt.tight_layout()
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†ç¾¤è¦–è¦ºåŒ–å·²ä¿å­˜: {viz_path}")
    
    return viz_path

def main():
    """ä¸»å‡½æ•¸"""
    print("="*60)
    print("ğŸš€ æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤ (Jenks Natural Breaks)")
    print("="*60)
    
    # 1. è¼‰å…¥æ•¸æ“š
    X, districts, features = load_data()
    
    # 2. åŸ·è¡Œåˆ†ç¾¤
    results_df, X_tsne, cluster_labels = perform_clustering(X, districts)
    
    # 3. ä¿å­˜çµæœ
    output_path = save_results(results_df)
    
    # 4. å‰µå»ºè¦–è¦ºåŒ–
    viz_path = create_visualization(results_df, X_tsne)
    
    print("\nâœ… åˆ†ç¾¤åŠæ¨™ç±¤å®Œæˆ!")
    print(f"  - åˆ†ç¾¤çµæœ: {output_path}")
    print(f"  - è¦–è¦ºåŒ–åœ–: {viz_path}")

if __name__ == "__main__":
    main() 