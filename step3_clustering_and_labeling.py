"""
æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤

ä½¿ç”¨t-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡é€²è¡Œåˆ†ç¾¤
è¼ªå»“ä¿‚æ•¸>0.7ä¸”åˆ†ç¾¤æ•¸=3

è¼¸å‡º:
- åˆ†ç¾¤çµæœCSV
- åˆ†ç¾¤è¦–è¦ºåŒ–åœ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import matplotlib
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# åƒæ•¸è¨­å®š
OUTPUT_DIR = 'output'
RANDOM_STATE = 42

def load_data():
    """è¼‰å…¥ç‰¹å¾µæ•¸æ“š"""
    print("ğŸ“‚ è¼‰å…¥ç‰¹å¾µæ•¸æ“š...")
    
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    
    # æå–è¡Œæ”¿å€å’Œç‰¹å¾µ
    districts = df['å€åŸŸåˆ¥'].tolist()
    X = df.drop('å€åŸŸåˆ¥', axis=1).values
    
    print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(districts)} å€‹è¡Œæ”¿å€, {X.shape[1]} å€‹ç‰¹å¾µ")
    print(f"  ç‰¹å¾µåˆ—è¡¨: {', '.join(df.columns[1:])}")
    
    return X, districts, df.columns[1:].tolist()

def perform_clustering(X, districts):
    """åŸ·è¡Œt-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡"""
    print("\nğŸ” åŸ·è¡Œt-SNEé™ç¶­å’ŒWardå±¤æ¬¡èšé¡...")
    
    # t-SNEé™ç¶­
    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
    X_tsne = tsne.fit_transform(X)
    print(f"âœ… t-SNEé™ç¶­å®Œæˆ: {X.shape} â†’ {X_tsne.shape}")
    
    # Wardå±¤æ¬¡èšé¡
    ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
    cluster_labels = ward.fit_predict(X_tsne)
    
    # è¨ˆç®—è¼ªå»“ä¿‚æ•¸
    silhouette = silhouette_score(X_tsne, cluster_labels)
    print(f"âœ… Wardå±¤æ¬¡èšé¡å®Œæˆ: 3å€‹é›†ç¾¤, è¼ªå»“ä¿‚æ•¸={silhouette:.3f}")
    
    # è¨ˆç®—æ¯å€‹é›†ç¾¤çš„ä¸­å¿ƒé»
    cluster_centers = np.array([X_tsne[cluster_labels == i].mean(axis=0) for i in range(3)])
    
    # è¨ˆç®—æ¯å€‹é»åˆ°æ¯å€‹é›†ç¾¤ä¸­å¿ƒçš„è·é›¢
    distances = pairwise_distances(X_tsne, cluster_centers)
    
    # ä½¿ç”¨æ›´åˆç†çš„æ©Ÿç‡è¨ˆç®—æ–¹æ³•
    def calculate_probabilities(distances):
        # è¨ˆç®—åˆ°æœ€è¿‘é›†ç¾¤çš„ç›¸å°è·é›¢
        min_distances = distances.min(axis=1, keepdims=True)
        relative_distances = distances / (min_distances + 1e-6)
        
        # ä½¿ç”¨æº«åº¦èª¿ç¯€çš„softmaxï¼ˆæº«åº¦è¼ƒé«˜ï¼Œæ©Ÿç‡åˆ†å¸ƒè¼ƒå¹³æ»‘ï¼‰
        temperature = 3.0
        exp_values = np.exp(-relative_distances / temperature)
        probabilities = exp_values / exp_values.sum(axis=1, keepdims=True)
        return probabilities
    
    cluster_proba = calculate_probabilities(distances)
    max_probas = np.max(cluster_proba, axis=1)
    
    # è¨ˆç®—ä¸ç¢ºå®šåº¦ï¼šä½¿ç”¨ç†µçš„æ¦‚å¿µ
    def calculate_uncertainty(probabilities):
        # é¿å…log(0)
        prob_safe = np.clip(probabilities, 1e-10, 1.0)
        entropy = -np.sum(probabilities * np.log(prob_safe), axis=1)
        # æ­£è¦åŒ–entropyåˆ°[0,1]ç¯„åœï¼ˆæœ€å¤§entropyç‚ºlog(3)ï¼‰
        max_entropy = np.log(3)
        normalized_entropy = entropy / max_entropy
        return normalized_entropy
    
    uncertainties = calculate_uncertainty(cluster_proba)
    
    # åˆ†æå„é›†ç¾¤
    for cluster_id in range(3):
        cluster_districts = [districts[i] for i in range(len(districts)) if cluster_labels[i] == cluster_id]
        print(f"  é›†ç¾¤ {cluster_id}: {len(cluster_districts)} å€‹è¡Œæ”¿å€ - {', '.join(cluster_districts)}")
    
    # ğŸ”§ ä¿®æ­£ï¼šåŸºæ–¼æ‰€å¾—æ°´å¹³é€²è¡Œæ½›åŠ›ç­‰ç´šæ˜ å°„
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“šä»¥ç²å–æ‰€å¾—ä¿¡æ¯
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    income_data = df.set_index('å€åŸŸåˆ¥')['æ‰€å¾—_median_household_income'].to_dict()
    
    # è¨ˆç®—å„é›†ç¾¤çš„å¹³å‡æ‰€å¾—
    cluster_incomes = []
    for cluster_id in range(3):
        cluster_mask = cluster_labels == cluster_id
        cluster_districts_list = [districts[i] for i in range(len(districts)) if cluster_mask[i]]
        cluster_income = np.mean([income_data[district] for district in cluster_districts_list])
        cluster_incomes.append(cluster_income)
        print(f"  é›†ç¾¤ {cluster_id} å¹³å‡æ‰€å¾—: {cluster_income:,.0f} å…ƒ")
    
    # æŒ‰å¹³å‡æ‰€å¾—æ’åºï¼ˆé™åºï¼šé«˜æ‰€å¾—=é«˜æ½›åŠ›ï¼‰
    cluster_order = np.argsort(cluster_incomes)[::-1]  # é™åºæ’åˆ—
    potential_mapping = {
        cluster_order[0]: 'é«˜æ½›åŠ›',
        cluster_order[1]: 'ä¸­æ½›åŠ›',
        cluster_order[2]: 'ä½æ½›åŠ›'
    }
    
    print(f"âœ… åŸºæ–¼æ‰€å¾—çš„æ½›åŠ›ç­‰ç´šæ˜ å°„: {potential_mapping}")
    print(f"  å„é›†ç¾¤å¹³å‡æ‰€å¾—æ’åº: {[f'{cluster_incomes[i]:,.0f}å…ƒ' for i in cluster_order]}")
    
    # å‰µå»ºçµæœDataFrame
    results_df = pd.DataFrame({
        'è¡Œæ”¿å€': districts,
        'é›†ç¾¤ç·¨è™Ÿ': cluster_labels,
        'æ½›åŠ›ç­‰ç´š': [potential_mapping[label] for label in cluster_labels],
        'tsne_x': X_tsne[:, 0],
        'tsne_y': X_tsne[:, 1],
        'åˆ†ç¾¤æ©Ÿç‡': max_probas,
        'ä¸ç¢ºå®šåº¦': uncertainties
    })
    
    return results_df, X_tsne, cluster_labels

def save_results(results_df):
    """ä¿å­˜åˆ†ç¾¤çµæœ"""
    print("\nğŸ’¾ ä¿å­˜åˆ†ç¾¤çµæœ...")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜åˆ†ç¾¤çµæœCSV
    output_path = os.path.join(OUTPUT_DIR, 'clustering_results.csv')
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ… åˆ†ç¾¤çµæœå·²ä¿å­˜: {output_path}")
    
    return output_path

def create_visualization(results_df, X_tsne):
    """å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–"""
    print("\nğŸ¨ å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–...")
    
    # é¡è‰²æ˜ å°„
    level_colors = {'é«˜æ½›åŠ›': 'red', 'ä¸­æ½›åŠ›': 'orange', 'ä½æ½›åŠ›': 'blue'}
    
    # å‰µå»ºç•«å¸ƒ
    plt.figure(figsize=(10, 8))
    
    # ç¹ªè£½æ•£é»åœ–
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        if len(level_data) > 0:
            plt.scatter(
                level_data['tsne_x'], level_data['tsne_y'],
                c=level_colors.get(level, 'gray'),
                label=level,
                s=120,
                alpha=0.8
            )
    
    # æ¨™è¨»è¡Œæ”¿å€åç¨±
    for i, row in results_df.iterrows():
        plt.annotate(
            row['è¡Œæ”¿å€'],
            (row['tsne_x'], row['tsne_y']),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=10,
            ha='left'
        )
    
    # æ·»åŠ æ¨™é¡Œå’Œåœ–ä¾‹
    plt.title('æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤çµæœ (t-SNEé™ç¶­)', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # ä¿å­˜åœ–ç‰‡
    viz_path = os.path.join(OUTPUT_DIR, 'clustering_visualization.png')
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†ç¾¤è¦–è¦ºåŒ–å·²ä¿å­˜: {viz_path}")
    
    return viz_path

def main():
    """ä¸»å‡½æ•¸"""
    print("="*60)
    print("ğŸš€ æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤")
    print("="*60)
    
    # 1. è¼‰å…¥æ•¸æ“š
    X, districts, features = load_data()
    
    # 2. åŸ·è¡Œåˆ†ç¾¤
    results_df, X_tsne, cluster_labels = perform_clustering(X, districts)
    
    # 3. ä¿å­˜çµæœ
    output_path = save_results(results_df)
    
    # 4. å‰µå»ºè¦–è¦ºåŒ–
    viz_path = create_visualization(results_df, X_tsne)
    
    print("\nâœ… åˆ†ç¾¤åŠæ¨™ç±¤å®Œæˆ!")
    print(f"  - åˆ†ç¾¤çµæœ: {output_path}")
    print(f"  - è¦–è¦ºåŒ–åœ–: {viz_path}")

if __name__ == "__main__":
    main() 