"""
æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤

ç›´æ¥å°æ¨™æº–åŒ–ç‰¹å¾µé€²è¡ŒWardå±¤æ¬¡èšé¡
æ ¹æ“šclusterå¹³å‡æ½›åŠ›åˆ†æ•¸åˆ†é…é«˜/ä¸­/ä½æ¨™ç±¤

è¼¸å‡º:
- åˆ†ç¾¤çµæœCSV
- åˆ†ç¾¤è¦–è¦ºåŒ–åœ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import matplotlib
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import StandardScaler

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# åƒæ•¸è¨­å®š
OUTPUT_DIR = 'output'
RANDOM_STATE = 42

def load_data():
    """è¼‰å…¥ç‰¹å¾µæ•¸æ“š"""
    print("ğŸ“‚ è¼‰å…¥ç‰¹å¾µæ•¸æ“š...")
    
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    
    # æå–è¡Œæ”¿å€å’Œç‰¹å¾µ
    districts = df['å€åŸŸåˆ¥'].tolist()
    X = df.drop('å€åŸŸåˆ¥', axis=1).values
    
    print(f"âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(districts)} å€‹è¡Œæ”¿å€, {X.shape[1]} å€‹ç‰¹å¾µ")
    print(f"  ç‰¹å¾µåˆ—è¡¨: {', '.join(df.columns[1:])}")
    
    return X, districts, df.columns[1:].tolist()

def calculate_composite_score(districts):
    """è¨ˆç®—ç¶œåˆç™¼å±•æ½›åŠ›åˆ†æ•¸"""
    print("\nğŸ“Š è¨ˆç®—ç¶œåˆç™¼å±•æ½›åŠ›åˆ†æ•¸...")
    
    # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
    df = pd.read_csv('output/taoyuan_features_enhanced.csv')
    df = df.set_index('å€åŸŸåˆ¥')
    
    # ç‰¹å¾µæ¬Šé‡è¨­å®šï¼ˆåŸºæ–¼é ˜åŸŸçŸ¥è­˜ï¼‰
    weights = {
        'æ‰€å¾—_median_household_income': 0.35,      # ç¶“æ¿Ÿæ°´å¹³ - æœ€é‡è¦
        'tertiary_industry_ratio': 0.25,          # ç”¢æ¥­çµæ§‹ 
        'medical_index': 0.20,                    # é†«ç™‚è³‡æº
        'äººå£_working_age_ratio': 0.15,           # äººåŠ›è³‡æº
        'å•†æ¥­_hhi_index': 0.05                    # å•†æ¥­é›†ä¸­åº¦
    }
    
    print(f"  ç‰¹å¾µæ¬Šé‡è¨­å®š: {weights}")
    
    # æ¨™æº–åŒ–æ•¸æ“š
    normalized_data = df.copy()
    for feature in weights.keys():
        if feature in df.columns:
            min_val = df[feature].min()
            max_val = df[feature].max()
            normalized_data[feature] = (df[feature] - min_val) / (max_val - min_val)
    
    # è¨ˆç®—åŠ æ¬Šç¶œåˆåˆ†æ•¸
    composite_scores = {}
    for district in districts:
        score = 0
        for feature, weight in weights.items():
            if feature in normalized_data.columns:
                score += normalized_data.loc[district, feature] * weight
        composite_scores[district] = score
    
    print(f"âœ… ç¶œåˆåˆ†æ•¸è¨ˆç®—å®Œæˆ")
    for district, score in sorted(composite_scores.items(), key=lambda x: x[1], reverse=True):
        print(f"  {district}: {score:.3f}")
    
    return composite_scores

def perform_clustering(X, districts):
    """å°æ¨™æº–åŒ–ç‰¹å¾µé€²è¡ŒWardå±¤æ¬¡èšé¡"""
    print("\nğŸ” å°æ¨™æº–åŒ–ç‰¹å¾µé€²è¡ŒWardå±¤æ¬¡èšé¡...")
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print(f"âœ… ç‰¹å¾µæ¨™æº–åŒ–å®Œæˆ: {X.shape}")
    
    # Wardå±¤æ¬¡èšé¡ï¼ˆç›´æ¥å°æ¨™æº–åŒ–ç‰¹å¾µé€²è¡Œï¼‰
    ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
    cluster_labels = ward.fit_predict(X_scaled)
    
    # è¨ˆç®—è¼ªå»“ä¿‚æ•¸
    silhouette = silhouette_score(X_scaled, cluster_labels)
    print(f"âœ… Wardå±¤æ¬¡èšé¡å®Œæˆ: 3å€‹é›†ç¾¤, è¼ªå»“ä¿‚æ•¸={silhouette:.3f}")
    
    # è¨ˆç®—ç¶œåˆæ½›åŠ›åˆ†æ•¸
    composite_scores = calculate_composite_score(districts)
    
    # åˆ†æå„é›†ç¾¤åŠå…¶å¹³å‡æ½›åŠ›åˆ†æ•¸
    cluster_info = {}
    for cluster_id in range(3):
        cluster_mask = cluster_labels == cluster_id
        cluster_districts = [districts[i] for i in range(len(districts)) if cluster_mask[i]]
        cluster_scores = [composite_scores[district] for district in cluster_districts]
        avg_score = np.mean(cluster_scores)
        
        cluster_info[cluster_id] = {
            'districts': cluster_districts,
            'avg_score': avg_score,
            'scores': cluster_scores
        }
        
        print(f"  é›†ç¾¤ {cluster_id}: {len(cluster_districts)} å€‹è¡Œæ”¿å€")
        print(f"    è¡Œæ”¿å€: {', '.join(cluster_districts)}")
        print(f"    å¹³å‡æ½›åŠ›åˆ†æ•¸: {avg_score:.3f}")
    
    # æ ¹æ“šclusterå¹³å‡åˆ†æ•¸åˆ†é…æ½›åŠ›ç­‰ç´š
    print(f"\nğŸ¯ æ ¹æ“šclusterå¹³å‡æ½›åŠ›åˆ†æ•¸åˆ†é…ç­‰ç´š...")
    
    # æŒ‰å¹³å‡åˆ†æ•¸æ’åºclusterï¼ˆé™åºï¼šé«˜åˆ†æ•¸=é«˜æ½›åŠ›ï¼‰
    sorted_clusters = sorted(cluster_info.items(), key=lambda x: x[1]['avg_score'], reverse=True)
    
    potential_mapping = {
        sorted_clusters[0][0]: 'é«˜æ½›åŠ›',  # æœ€é«˜å¹³å‡åˆ†æ•¸
        sorted_clusters[1][0]: 'ä¸­æ½›åŠ›',  # ä¸­ç­‰å¹³å‡åˆ†æ•¸
        sorted_clusters[2][0]: 'ä½æ½›åŠ›'   # æœ€ä½å¹³å‡åˆ†æ•¸
    }
    
    print(f"âœ… Clusteræ½›åŠ›ç­‰ç´šåˆ†é…:")
    for cluster_id, level in potential_mapping.items():
        info = cluster_info[cluster_id]
        print(f"  é›†ç¾¤ {cluster_id} â†’ {level}")
        print(f"    å¹³å‡åˆ†æ•¸: {info['avg_score']:.3f}")
        print(f"    è¡Œæ”¿å€: {', '.join(info['districts'])}")
    
    # ç‚ºt-SNEè¦–è¦ºåŒ–è¨ˆç®—åº§æ¨™ï¼ˆåƒ…ç”¨æ–¼è¦–è¦ºåŒ–ï¼‰
    print(f"\nğŸ¨ è¨ˆç®—t-SNEåº§æ¨™ç”¨æ–¼è¦–è¦ºåŒ–...")
    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=3, max_iter=1000)
    X_tsne = tsne.fit_transform(X_scaled)
    print(f"âœ… t-SNEè¦–è¦ºåŒ–åº§æ¨™è¨ˆç®—å®Œæˆ")
    
    # è¨ˆç®—åˆ†ç¾¤æ©Ÿç‡å’Œä¸ç¢ºå®šåº¦
    cluster_centers = np.array([X_scaled[cluster_labels == i].mean(axis=0) for i in range(3)])
    distances = pairwise_distances(X_scaled, cluster_centers)
    
    def calculate_probabilities(distances):
        min_distances = distances.min(axis=1, keepdims=True)
        relative_distances = distances / (min_distances + 1e-6)
        temperature = 3.0
        exp_values = np.exp(-relative_distances / temperature)
        probabilities = exp_values / exp_values.sum(axis=1, keepdims=True)
        return probabilities
    
    cluster_proba = calculate_probabilities(distances)
    max_probas = np.max(cluster_proba, axis=1)
    
    def calculate_uncertainty(probabilities):
        prob_safe = np.clip(probabilities, 1e-10, 1.0)
        entropy = -np.sum(probabilities * np.log(prob_safe), axis=1)
        max_entropy = np.log(3)
        normalized_entropy = entropy / max_entropy
        return normalized_entropy
    
    uncertainties = calculate_uncertainty(cluster_proba)
    
    # å‰µå»ºçµæœDataFrame
    results_df = pd.DataFrame({
        'è¡Œæ”¿å€': districts,
        'é›†ç¾¤ç·¨è™Ÿ': cluster_labels,
        'æ½›åŠ›ç­‰ç´š': [potential_mapping[label] for label in cluster_labels],
        'ç¶œåˆåˆ†æ•¸': [composite_scores[district] for district in districts],
        'tsne_x': X_tsne[:, 0],
        'tsne_y': X_tsne[:, 1],
        'åˆ†ç¾¤æ©Ÿç‡': max_probas,
        'ä¸ç¢ºå®šåº¦': uncertainties
    })
    
    # è¼¸å‡ºçµ±è¨ˆä¿¡æ¯
    print(f"\nğŸ“ˆ æœ€çµ‚åˆ†é¡çµ±è¨ˆ:")
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        count = len(level_data)
        avg_score = level_data['ç¶œåˆåˆ†æ•¸'].mean()
        districts_list = level_data['è¡Œæ”¿å€'].tolist()
        print(f"  {level}: {count}å€‹è¡Œæ”¿å€, å¹³å‡åˆ†æ•¸={avg_score:.3f}")
        print(f"    è¡Œæ”¿å€: {', '.join(districts_list)}")
    
    return results_df, X_tsne, cluster_labels, silhouette

def save_results(results_df):
    """ä¿å­˜åˆ†ç¾¤çµæœ"""
    print("\nğŸ’¾ ä¿å­˜åˆ†ç¾¤çµæœ...")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜åˆ†ç¾¤çµæœCSV
    output_path = os.path.join(OUTPUT_DIR, 'clustering_results.csv')
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"âœ… åˆ†ç¾¤çµæœå·²ä¿å­˜: {output_path}")
    
    return output_path

def create_visualization(results_df, X_tsne, silhouette):
    """å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–"""
    print("\nğŸ¨ å‰µå»ºåˆ†ç¾¤è¦–è¦ºåŒ–...")
    
    # é¡è‰²æ˜ å°„
    level_colors = {'é«˜æ½›åŠ›': 'red', 'ä¸­æ½›åŠ›': 'orange', 'ä½æ½›åŠ›': 'blue'}
    
    # å‰µå»ºç•«å¸ƒ
    plt.figure(figsize=(15, 6))
    
    # å­åœ–1: t-SNEèšé¡çµæœï¼ˆåƒ…ç”¨æ–¼è¦–è¦ºåŒ–ï¼‰
    plt.subplot(1, 3, 1)
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        if len(level_data) > 0:
            plt.scatter(
                level_data['tsne_x'], level_data['tsne_y'],
                c=level_colors.get(level, 'gray'),
                label=level,
                s=120,
                alpha=0.8
            )
    
    # æ¨™è¨»è¡Œæ”¿å€åç¨±
    for i, row in results_df.iterrows():
        plt.annotate(
            row['è¡Œæ”¿å€'],
            (row['tsne_x'], row['tsne_y']),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=8,
            ha='left'
        )
    
    plt.title('t-SNEè¦–è¦ºåŒ–\n(åŸºæ–¼Wardèšé¡çµæœ)', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # å­åœ–2: ç¶œåˆåˆ†æ•¸åˆ†å¸ƒ
    plt.subplot(1, 3, 2)
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        level_data = results_df[results_df['æ½›åŠ›ç­‰ç´š'] == level]
        
        plt.hist(level_data['ç¶œåˆåˆ†æ•¸'], 
                alpha=0.7, 
                color=level_colors[level], 
                label=level,
                bins=5)
    
    plt.title('ç¶œåˆåˆ†æ•¸åˆ†å¸ƒ\n(åŸºæ–¼Clusterå¹³å‡åˆ†æ•¸åˆ†é¡)', fontsize=12, fontweight='bold')
    plt.xlabel('ç¶œåˆç™¼å±•æ½›åŠ›åˆ†æ•¸')
    plt.ylabel('è¡Œæ”¿å€æ•¸é‡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­åœ–3: Clusterçµ±è¨ˆ
    plt.subplot(1, 3, 3)
    cluster_stats = results_df.groupby(['é›†ç¾¤ç·¨è™Ÿ', 'æ½›åŠ›ç­‰ç´š']).agg({
        'ç¶œåˆåˆ†æ•¸': ['count', 'mean']
    }).round(3)
    
    clusters = results_df['é›†ç¾¤ç·¨è™Ÿ'].unique()
    levels = []
    counts = []
    scores = []
    colors = []
    
    for cluster_id in sorted(clusters):
        cluster_data = results_df[results_df['é›†ç¾¤ç·¨è™Ÿ'] == cluster_id]
        level = cluster_data['æ½›åŠ›ç­‰ç´š'].iloc[0]
        count = len(cluster_data)
        score = cluster_data['ç¶œåˆåˆ†æ•¸'].mean()
        
        levels.append(f"Cluster {cluster_id}\n({level})")
        counts.append(count)
        scores.append(score)
        colors.append(level_colors[level])
    
    bars = plt.bar(levels, scores, color=colors, alpha=0.7)
    
    # åœ¨æŸ±ç‹€åœ–ä¸Šæ¨™è¨»æ•¸é‡
    for i, (bar, count) in enumerate(zip(bars, counts)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{count}å€', ha='center', va='bottom', fontsize=10)
    
    plt.title(f'å„Clusterå¹³å‡æ½›åŠ›åˆ†æ•¸\n(è¼ªå»“ä¿‚æ•¸: {silhouette:.3f})', fontsize=12, fontweight='bold')
    plt.ylabel('å¹³å‡ç¶œåˆåˆ†æ•¸')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3, axis='y')
    
    # ä¿å­˜åœ–ç‰‡
    viz_path = os.path.join(OUTPUT_DIR, 'clustering_visualization.png')
    plt.tight_layout()
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†ç¾¤è¦–è¦ºåŒ–å·²ä¿å­˜: {viz_path}")
    
    return viz_path

def main():
    """ä¸»å‡½æ•¸"""
    print("="*60)
    print("ğŸš€ æ¡ƒåœ’å¸‚è¡Œæ”¿å€åˆ†ç¾¤åŠæ¨™ç±¤ (Wardèšé¡ + Clusterå¹³å‡åˆ†æ•¸åˆ†é¡)")
    print("="*60)
    
    # 1. è¼‰å…¥æ•¸æ“š
    X, districts, features = load_data()
    
    # 2. åŸ·è¡Œåˆ†ç¾¤
    results_df, X_tsne, cluster_labels, silhouette = perform_clustering(X, districts)
    
    # 3. ä¿å­˜çµæœ
    output_path = save_results(results_df)
    
    # 4. å‰µå»ºè¦–è¦ºåŒ–
    viz_path = create_visualization(results_df, X_tsne, silhouette)
    
    print("\nâœ… åˆ†ç¾¤åŠæ¨™ç±¤å®Œæˆ!")
    print(f"  - åˆ†ç¾¤çµæœ: {output_path}")
    print(f"  - è¦–è¦ºåŒ–åœ–: {viz_path}")

if __name__ == "__main__":
    main() 