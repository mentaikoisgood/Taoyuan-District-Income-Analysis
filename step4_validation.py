"""
STEP 4 - 3ç´šJenksåˆ†ç´šé©—è­‰èˆ‡åˆ†æ

åŒ…å«ï¼š
1. è¼‰å…¥step3çš„3ç´šJenksåˆ†ç´šçµæœ
2. åˆ†ç´šè³ªé‡åˆ†æï¼ˆçµ„å…§å¤–æ–¹å·®ã€Fçµ±è¨ˆé‡ï¼‰
3. ç©©å®šæ€§æ¸¬è©¦ï¼ˆBootstrapï¼‰
4. ç‰¹å¾µåˆ†å¸ƒåˆ†æå’Œè¦–è¦ºåŒ–
5. ç¶œåˆè©•ä¼°å ±å‘Š

å°ˆæ³¨æ–¼é©—è­‰åˆ†ç´šæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç©©å®šæ€§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from scipy import stats
import json
import warnings
import os
import matplotlib

warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
matplotlib.rcParams['font.family'] = ['Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 10

OUTPUT_DIR = 'output'
RANDOM_STATE = 42

# å˜—è©¦å°å…¥Jenks
try:
    import jenkspy
    JENKS_AVAILABLE = True
except ImportError:
    JENKS_AVAILABLE = False
    print("âš ï¸ Jenksæœªå®‰è£ï¼Œéƒ¨åˆ†åŠŸèƒ½å—é™")

def load_classification_results():
    """è¼‰å…¥step3çš„åˆ†ç´šçµæœ"""
    print("ğŸ“‚ è¼‰å…¥3ç´šJenksåˆ†ç´šçµæœ...")
    
    try:
        # è¼‰å…¥åˆ†ç´šçµæœ
        results_df = pd.read_csv('output/3_level_jenks_results.csv')
        
        # è¼‰å…¥é…ç½®ä¿¡æ¯
        with open('output/3_level_jenks_config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    
        # è¼‰å…¥åŸå§‹ç‰¹å¾µæ•¸æ“š
        df = pd.read_csv('output/taoyuan_features_enhanced.csv')
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(results_df)} å€‹è¡Œæ”¿å€çš„åˆ†ç´šçµæœ")
        print(f"åˆ†ç´šæ–¹æ³•: {config['method']}")
        print(f"åˆ†å‰²é»: {config['breaks']}") # Breaks should be 0-10 scale from step3
        
        return results_df, config, df
        
    except FileNotFoundError as e:
        print(f"âŒ ç„¡æ³•è¼‰å…¥åˆ†ç´šçµæœ: {e}")
        print("è«‹å…ˆåŸ·è¡Œ step3_ranking_classification.py")
        return None, None, None

def analyze_classification_quality(scores, labels):
    """åˆ†æ3ç´šåˆ†é¡è³ªé‡"""
    print("\nğŸ“ˆ 3ç´šåˆ†é¡è³ªé‡åˆ†æ...")
    
    def calculate_within_group_variance(scores, labels):
        """è¨ˆç®—çµ„å…§æ–¹å·®"""
        total_variance = 0
        unique_labels = pd.Series(labels).unique()
        
        for label in unique_labels:
            mask = pd.Series(labels) == label
            group_scores = scores[mask]
            if len(group_scores) > 1:
                total_variance += np.var(group_scores) * len(group_scores)
        
        return total_variance / len(scores)
    
    def calculate_between_group_variance(scores, labels):
        """è¨ˆç®—çµ„é–“æ–¹å·®"""
        overall_mean = np.mean(scores)
        total_variance = 0
        unique_labels = pd.Series(labels).unique()
        
        for label in unique_labels:
            mask = pd.Series(labels) == label
            group_scores = scores[mask]
            group_mean = np.mean(group_scores)
            total_variance += (group_mean - overall_mean) ** 2 * len(group_scores)
        
        return total_variance / len(scores)
    
    # è¨ˆç®—çµ„å…§å¤–æ–¹å·®
    within_var = calculate_within_group_variance(scores, labels)
    between_var = calculate_between_group_variance(scores, labels)
    
    # Fçµ±è¨ˆé‡å’Œæ•ˆæ‡‰å¤§å°
    f_stat = between_var / within_var if within_var > 0 else np.inf
    eta_squared = between_var / (between_var + within_var)
    
    # å„ç´šåˆ¥çµ±è¨ˆ
    level_stats = {}
    unique_labels = pd.Series(labels).unique()
    
    for label in unique_labels:
        mask = pd.Series(labels) == label
        group_scores = scores[mask]
        level_stats[label] = {
            'count': len(group_scores),
            'mean': np.mean(group_scores),
            'std': np.std(group_scores),
            'min': np.min(group_scores),
            'max': np.max(group_scores)
        }
    
    quality_metrics = {
        'within_variance': within_var,
        'between_variance': between_var,
        'f_statistic': f_stat,
        'eta_squared': eta_squared,
        'level_stats': level_stats
    }
    
    print(f"  è³ªé‡æŒ‡æ¨™:")
    print(f"    çµ„å…§æ–¹å·®: {within_var:.2f}")
    print(f"    çµ„é–“æ–¹å·®: {between_var:.2f}")
    print(f"    Fçµ±è¨ˆé‡: {f_stat:.2f}")
    print(f"    æ•ˆæ‡‰å¤§å°(Î·Â²): {eta_squared:.3f}")
    
    print(f"\n  å„ç´šåˆ¥çµ±è¨ˆ:")
    for label, stats in level_stats.items():
        print(f"    {label}: {stats['count']}å€‹å€åŸŸ, å¹³å‡{stats['mean']:.1f}Â±{stats['std']:.1f}")
    
    return quality_metrics

def create_comprehensive_visualization(results_df, df, feature_names, quality_metrics, stability_results, config):
    """å‰µå»ºç¶œåˆé©—è­‰è¦–è¦ºåŒ–"""
    print("\nğŸ¨ å‰µå»ºç¶œåˆé©—è­‰è¦–è¦ºåŒ–...")
    
    # è¨­å®š3ç´šé¡è‰² - èˆ‡HTMLä¸€è‡´çš„é…è‰²æ–¹æ¡ˆ
    colors = {'é«˜æ½›åŠ›': '#EB7062', 'ä¸­æ½›åŠ›': '#F5B041', 'ä½æ½›åŠ›': '#5CACE2'}
    levels_order = ['ä½æ½›åŠ›', 'ä¸­æ½›åŠ›', 'é«˜æ½›åŠ›']
    
    # æº–å‚™æ•¸æ“š
    scores = results_df['ç¶œåˆåˆ†æ•¸'].values
    labels = results_df['3ç´šJenksåˆ†ç´š'].values
    districts = results_df['å€åŸŸåˆ¥'].tolist()
    
    # å‰µå»ºç•«å¸ƒ
    fig = plt.figure(figsize=(20, 16))
    
    # å­åœ–1: åˆ†ç´šæ•£é»åœ–
    plt.subplot(3, 4, 1)
    for level in levels_order:
        mask = labels == level
        if np.any(mask):
            plt.scatter(np.where(mask)[0], scores[mask], 
                       c=colors[level], label=level, s=120, alpha=0.8, edgecolors='black')
    
    plt.xlabel('è¡Œæ”¿å€ç´¢å¼•')
    plt.ylabel('ç¶œåˆåˆ†æ•¸ (0-10åˆ†åˆ¶)')
    plt.title('3ç´šJenksåˆ†ç´šçµæœ', fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ åˆ†å‰²ç·š
    if config['breaks']:
        for i, break_point in enumerate(config['breaks'][1:-1], 1):
            plt.axhline(y=break_point, color='red', linestyle='--', alpha=0.7)
    
    # å­åœ–2: æ’åæŸ±ç‹€åœ–
    plt.subplot(3, 4, 2)
    sorted_indices = np.argsort(scores)[::-1]
    sorted_scores = scores[sorted_indices]
    sorted_districts = [districts[i] for i in sorted_indices]
    sorted_labels = [labels[i] for i in sorted_indices]
    
    bars = plt.bar(range(len(sorted_scores)), sorted_scores, 
                   color=[colors[label] for label in sorted_labels], 
                   alpha=0.8, edgecolor='black')
    
    plt.xticks(range(len(sorted_districts)), sorted_districts, rotation=45, ha='right', fontsize=8)
    plt.ylabel('ç¶œåˆåˆ†æ•¸ (0-10åˆ†åˆ¶)')
    plt.title('åˆ†æ•¸æ’å', fontweight='bold')
    plt.grid(True, alpha=0.3, axis='y')
    
    # å­åœ–3: å„ç´šåˆ¥åˆ†å¸ƒ
    plt.subplot(3, 4, 3)
    level_counts = pd.Series(labels).value_counts()
    pie_colors = [colors[level] for level in level_counts.index]
    
    plt.pie(level_counts.values, labels=level_counts.index, 
            colors=pie_colors, autopct='%1.0f%%', startangle=90)
    plt.title('ç´šåˆ¥åˆ†å¸ƒ', fontweight='bold')
    
    # å­åœ–4: è³ªé‡æŒ‡æ¨™
    plt.subplot(3, 4, 4)
    if quality_metrics:
        metrics_names = ['çµ„å…§\næ–¹å·®', 'çµ„é–“\næ–¹å·®', 'Fçµ±è¨ˆé‡', 'æ•ˆæ‡‰\nå¤§å°']
        metrics_values = [
            quality_metrics['within_variance'],
            quality_metrics['between_variance'],
            quality_metrics['f_statistic'],
            quality_metrics['eta_squared']
        ]
        
        bars = plt.bar(metrics_names, metrics_values, color=['orange', 'green', 'blue', 'purple'], alpha=0.7)
        plt.ylabel('æŒ‡æ¨™å€¼')
        plt.title('åˆ†ç´šè³ªé‡æŒ‡æ¨™', fontweight='bold')
        plt.xticks(fontsize=8)
        plt.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, value in zip(bars, metrics_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.02, 
                    f'{value:.2f}', ha='center', va='bottom', fontsize=8)
    
    # å­åœ–5: ç‰¹å¾µç†±åŠ›åœ–
    plt.subplot(3, 4, 5)
    scaler = StandardScaler()
    X_std = scaler.fit_transform(df[feature_names])
    X_sorted = X_std[sorted_indices]
    
    im = plt.imshow(X_sorted.T, cmap='RdBu_r', aspect='auto')
    plt.colorbar(im, fraction=0.046, pad=0.04)
    plt.xticks(range(len(sorted_districts)), sorted_districts, rotation=45, ha='right', fontsize=6)
    plt.yticks(range(len(feature_names)), [name.replace('_', '\n') for name in feature_names], fontsize=8)
    plt.title('ç‰¹å¾µç†±åŠ›åœ–\n(æŒ‰åˆ†æ•¸æ’åº)', fontweight='bold')
    
    # å­åœ–6: ç©©å®šæ€§æ¸¬è©¦çµæœ
    plt.subplot(3, 4, 6)
    if stability_results:
        stability_data = [
            stability_results['score_correlations'],
            stability_results['ranking_correlations']
        ]
        labels_box = ['åˆ†æ•¸\nç›¸é—œæ€§', 'æ’å\nç›¸é—œæ€§']
        
        box_plot = plt.boxplot(stability_data, labels=labels_box, patch_artist=True)
        box_plot['boxes'][0].set_facecolor('lightblue')
        box_plot['boxes'][1].set_facecolor('lightgreen')
        
        plt.ylabel('ç©©å®šæ€§æŒ‡æ¨™')
        plt.title(f'Bootstrapç©©å®šæ€§\n({stability_results["stability_grade"]})', fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.xticks(fontsize=8)
        
        # æ·»åŠ å¹³å‡å€¼æ¨™ç·š
        plt.axhline(y=stability_results['avg_score_corr'], color='blue', linestyle='--', alpha=0.7)
        plt.axhline(y=stability_results['avg_ranking_corr'], color='green', linestyle='--', alpha=0.7)
    
    # å­åœ–7: å„ç´šåˆ¥ç®±å‹åœ–
    plt.subplot(3, 4, 7)
    level_scores = {}
    for level in levels_order:
        mask = labels == level
        if np.any(mask):
            level_scores[level] = scores[mask]
    
    if level_scores:
        box_data = [level_scores[level] for level in levels_order if level in level_scores]
        box_labels = [level for level in levels_order if level in level_scores]
        
        box_plot = plt.boxplot(box_data, labels=box_labels, patch_artist=True)
        for i, (box, level) in enumerate(zip(box_plot['boxes'], box_labels)):
            box.set_facecolor(colors[level])
            box.set_alpha(0.7)
        
        plt.ylabel('ç¶œåˆåˆ†æ•¸ (0-10åˆ†åˆ¶)')
        plt.title('å„ç´šåˆ¥åˆ†æ•¸åˆ†å¸ƒ', fontweight='bold')
        plt.xticks(rotation=45, ha='right', fontsize=8)
        plt.grid(True, alpha=0.3, axis='y')
    
    # å­åœ–8: ç‰¹å¾µè²¢ç»åˆ†æ
    plt.subplot(3, 4, 8)
    feature_contributions = []
    feature_labels = []
    
    # è¨ˆç®—å„ç‰¹å¾µå°åˆ†ç´šçš„è²¢ç»åº¦
    for feature in feature_names:
        feature_data = df[feature].values
        # è¨ˆç®—ç‰¹å¾µèˆ‡åˆ†æ•¸çš„ç›¸é—œæ€§
        corr = np.corrcoef(feature_data, scores)[0, 1]
        feature_contributions.append(abs(corr))
        feature_labels.append(feature.replace('_', '\n'))
    
    bars = plt.bar(range(len(feature_contributions)), feature_contributions, 
                   color='skyblue', alpha=0.7)
    plt.xticks(range(len(feature_labels)), feature_labels, rotation=45, ha='right', fontsize=7)
    plt.ylabel('ç‰¹å¾µé‡è¦æ€§\n(|ç›¸é—œä¿‚æ•¸|)')
    plt.title('ç‰¹å¾µè²¢ç»åˆ†æ', fontweight='bold')
    plt.grid(True, alpha=0.3, axis='y')
    
    # å­åœ–9-11: å„ç´šåˆ¥è©³ç´°çµ±è¨ˆ
    for i, level in enumerate(levels_order):
        plt.subplot(3, 4, 9+i)
        mask = labels == level
        if np.any(mask):
            level_districts = [districts[j] for j in range(len(districts)) if mask[j]]
            level_scores = scores[mask]
            
            plt.bar(range(len(level_districts)), level_scores, 
                   color=colors[level], alpha=0.7)
            plt.xticks(range(len(level_districts)), level_districts, 
                      rotation=45, ha='right', fontsize=8)
            plt.ylabel('åˆ†æ•¸')
            plt.title(f'{level}\n({len(level_districts)}å€‹å€åŸŸ)', fontweight='bold')
            plt.grid(True, alpha=0.3, axis='y')
            
            # æ·»åŠ å¹³å‡ç·š
            plt.axhline(y=np.mean(level_scores), color='red', linestyle='--', alpha=0.7)
    
    # æœ€å¾Œä¸€å€‹å­åœ–ï¼šç¸½çµçµ±è¨ˆ
    plt.subplot(3, 4, 12)
    plt.axis('off')
    
    # ä¿®å¾©f-stringæ ¼å¼å•é¡Œ
    score_corr_text = f"{stability_results['avg_score_corr']:.3f}" if stability_results else 'N/A'
    ranking_corr_text = f"{stability_results['avg_ranking_corr']:.3f}" if stability_results else 'N/A'
    grade_text = stability_results['stability_grade'] if stability_results else 'N/A'
    
    summary_text = f"""3ç´šJenksåˆ†ç´šç¸½çµ (0-10åˆ†åˆ¶)

ç´šåˆ¥åˆ†å¸ƒï¼š
â€¢ é«˜æ½›åŠ›ï¼š{sum(labels == 'é«˜æ½›åŠ›')} å€‹å€åŸŸ
â€¢ ä¸­æ½›åŠ›ï¼š{sum(labels == 'ä¸­æ½›åŠ›')} å€‹å€åŸŸ  
â€¢ ä½æ½›åŠ›ï¼š{sum(labels == 'ä½æ½›åŠ›')} å€‹å€åŸŸ

è³ªé‡æŒ‡æ¨™ï¼š
â€¢ Fçµ±è¨ˆé‡ï¼š{quality_metrics['f_statistic']:.2f}
â€¢ æ•ˆæ‡‰å¤§å°ï¼š{quality_metrics['eta_squared']:.3f}

ç©©å®šæ€§ï¼š
â€¢ ç­‰ç´šï¼š{grade_text}
â€¢ åˆ†æ•¸ç›¸é—œæ€§ï¼š{score_corr_text}
â€¢ æ’åç›¸é—œæ€§ï¼š{ranking_corr_text} 

çµè«–ï¼š3ç´šåˆ†é¡æä¾›è‰¯å¥½çš„
å€åˆ†åº¦èˆ‡å¯æ¥å—çš„ç©©å®šæ€§ã€‚
ï¼ˆè©³ç´°ç©©å®šæ€§è©•ä¼°è¦‹å ±å‘Šï¼‰
"""
    
    plt.text(0.1, 0.9, summary_text, fontsize=9, verticalalignment='top',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.5))
    
    plt.tight_layout()
    
    # ä¿å­˜åœ–ç‰‡
    viz_path = os.path.join(OUTPUT_DIR, '3_level_jenks_validation.png')
    plt.savefig(viz_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"âœ… é©—è­‰è¦–è¦ºåŒ–å·²ä¿å­˜: {viz_path}")
    return viz_path

def create_detailed_report(results_df, quality_metrics, stability_results, config, feature_names):
    """å‰µå»ºè©³ç´°é©—è­‰å ±å‘Š"""
    print("\nğŸ“ å‰µå»ºè©³ç´°é©—è­‰å ±å‘Š...")
    
    report = []
    report.append("# æ¡ƒåœ’å¸‚è¡Œæ”¿å€3ç´šJenksåˆ†ç´šé©—è­‰å ±å‘Š\n\n")
    
    report.append("## ğŸ“Š é©—è­‰æ¦‚è¿°\n")
    report.append(f"- **åˆ†ç´šæ–¹æ³•**: {config['method']}\n")
    report.append(f"- **åˆ†æå°è±¡**: {config['n_districts']} å€‹è¡Œæ”¿å€\n")
    report.append(f"- **è©•ä¼°ç‰¹å¾µ**: {len(feature_names)} å€‹æŒ‡æ¨™\n")
    report.append(f"- **åˆ†æ•¸ç¯„åœ**: {config['score_range'][0]:.1f} - {config['score_range'][1]:.1f} (0-10åˆ†åˆ¶)\\n\\n")
    
    report.append("## ğŸ¯ åˆ†ç´šçµæœçµ±è¨ˆ\n\n")
    level_counts = results_df['3ç´šJenksåˆ†ç´š'].value_counts()
    for level in ['é«˜æ½›åŠ›', 'ä¸­æ½›åŠ›', 'ä½æ½›åŠ›']:
        if level in level_counts:
            level_districts = results_df[results_df['3ç´šJenksåˆ†ç´š'] == level]['å€åŸŸåˆ¥'].tolist()
            level_scores = results_df[results_df['3ç´šJenksåˆ†ç´š'] == level]['ç¶œåˆåˆ†æ•¸'].tolist()
            
            report.append(f"### {level} ({level_counts[level]}å€‹å€åŸŸ)\n")
            for district, score in zip(level_districts, level_scores):
                report.append(f"- **{district}**: {score:.1f}åˆ†\\n")
            
            avg_score = np.mean(level_scores)
            std_score = np.std(level_scores)
            report.append(f"- å¹³å‡åˆ†æ•¸: {avg_score:.1f} Â± {std_score:.1f}\n\n")
    
    if quality_metrics:
        report.append("## ğŸ“ˆ åˆ†ç´šè³ªé‡åˆ†æ\n\n")
        report.append("### çµ±è¨ˆæŒ‡æ¨™\n")
        report.append(f"- **Fçµ±è¨ˆé‡**: {quality_metrics['f_statistic']:.2f}\n")
        report.append(f"- **æ•ˆæ‡‰å¤§å°(Î·Â²)**: {quality_metrics['eta_squared']:.3f}\n")
        report.append(f"- **çµ„é–“æ–¹å·®**: {quality_metrics['between_variance']:.2f}\n")
        report.append(f"- **çµ„å…§æ–¹å·®**: {quality_metrics['within_variance']:.2f}\n")
        report.append(f"- **æ–¹å·®æ¯”**: {quality_metrics['between_variance']/quality_metrics['within_variance']:.2f}\n\n")
        
        # è³ªé‡è©•ç´š
        if quality_metrics['f_statistic'] > 10:
            quality_grade = "å„ªç§€"
        elif quality_metrics['f_statistic'] > 5:
            quality_grade = "è‰¯å¥½"
        elif quality_metrics['f_statistic'] > 2:
            quality_grade = "ä¸­ç­‰"
        else:
            quality_grade = "éœ€æ”¹é€²"
        
        report.append(f"### è³ªé‡è©•ç´š: **{quality_grade}**\n\n")
    
    if stability_results:
        report.append("## ğŸ”„ ç©©å®šæ€§é©—è­‰\n\n")
        report.append(f"- **ç©©å®šæ€§ç­‰ç´š**: {stability_results['stability_grade']}\n")
        report.append(f"- **åˆ†æ•¸ç›¸é—œæ€§**: {stability_results['avg_score_corr']:.3f} Â± {np.std(stability_results['score_correlations']):.3f}\n")
        report.append(f"- **æ’åç›¸é—œæ€§**: {stability_results['avg_ranking_corr']:.3f} Â± {np.std(stability_results['ranking_correlations']):.3f}\n")
        report.append(f"- **æ¨™ç±¤ä¸€è‡´æ€§**: {stability_results['avg_agreement']:.3f} Â± {np.std(stability_results['label_agreements']):.3f}\n")
        report.append(f"- **Bootstrapæ¬¡æ•¸**: {len(stability_results['score_correlations'])}\n\n")
    
    report.append("## ğŸ† æœ€çµ‚æ’å\n\n")
    report.append("| æ’å | å€åŸŸ | åˆ†æ•¸(0-10) | ç´šåˆ¥ |\\n")
    report.append("|------|------|------------|------|\\n")
    
    for _, row in results_df.iterrows():
        report.append(f"| {row['æ’å']} | {row['å€åŸŸåˆ¥']} | {row['ç¶œåˆåˆ†æ•¸']:.1f} | {row['3ç´šJenksåˆ†ç´š']} |\\n")
    
    report.append("\n## ğŸ’¡ é©—è­‰çµè«–\n\n")
    
    if quality_metrics and stability_results:
        report.append(f"3ç´šJenksåˆ†ç´šæ–¹æ³•åœ¨æ¡ƒåœ’å¸‚13å€‹è¡Œæ”¿å€çš„åˆ†æä¸­è¡¨ç¾**{stability_results['stability_grade']}**ï¼š\n\n")
        report.append("### å„ªå‹¢\n")
        report.append("1. **çµ±è¨ˆé¡¯è‘—æ€§ä½³**: Fçµ±è¨ˆé‡é”åˆ°{:.2f}ï¼Œé¡¯ç¤ºåˆ†ç´šå·®ç•°é¡¯è‘—\n".format(quality_metrics['f_statistic']))
        report.append("2. **ç©©å®šæ€§å„ªè‰¯**: Bootstrapæ¸¬è©¦é¡¯ç¤ºé«˜åº¦ç©©å®šæ€§\n")
        report.append("3. **å¯¦ç”¨æ€§å¼·**: 3å€‹ç´šåˆ¥æä¾›åˆé©çš„æ”¿ç­–å€åˆ†åº¦\n")
        report.append("4. **ç§‘å­¸æ€§ä½³**: åŸºæ–¼æ•¸æ“šé©…å‹•çš„è‡ªç„¶æ–·é»\n")
        report.append("5. **å¯é‡ç¾æ€§é«˜**: é¿å…éš¨æ©Ÿæ€§å•é¡Œ\n")
        report.append("6. **å°æ¨£æœ¬é©æ‡‰**: ç‰¹åˆ¥é©åˆå°æ¨£æœ¬åˆ†æ\n\n")
        
        report.append("### æ‡‰ç”¨å»ºè­°\n")
        report.append("- **é«˜æ½›åŠ›å€åŸŸ**: é‡é»æŠ•è³‡å’Œå„ªåŒ–é…ç½®ï¼Œç™¼æ®å¼•é ˜ä½œç”¨\n")
        report.append("- **ä¸­æ½›åŠ›å€åŸŸ**: åŠ å¼·ç™¼å±•æ”¯æŒï¼ŒæŒ–æ˜æ½›åŠ›\n")
        report.append("- **ä½æ½›åŠ›å€åŸŸ**: åŸºç¤å»ºè¨­å’Œæ”¿ç­–å‚¾æ–œï¼Œæ”¹å–„æ¢ä»¶\n")
    
    # ä¿å­˜å ±å‘Š
    report_path = os.path.join(OUTPUT_DIR, '3_level_jenks_validation_report.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.writelines(report)
    
    print(f"âœ… é©—è­‰å ±å‘Šå·²ä¿å­˜: {report_path}")
    return report_path

def sensitivity_analysis(df, feature_names, base_feature_properties, results_df, n_variations=5):
    """æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ"""
    print(f"\nğŸ” æ¬Šé‡æ•æ„Ÿåº¦åˆ†æ (Â±0.05è®Šå‹•)...")
    
    if not JENKS_AVAILABLE:
        print("âŒ Jenksä¸å¯ç”¨ï¼Œè·³éæ•æ„Ÿåº¦åˆ†æ")
        return None
    
    import itertools
    from collections import defaultdict
    
    base_scores = results_df['ç¶œåˆåˆ†æ•¸'].values
    base_ranking = results_df['æ’å'].values
    districts = results_df['å€åŸŸåˆ¥'].tolist()
    
    # æ¬Šé‡è®Šå‹•è¨­å®š
    weight_delta = 0.05
    sensitivity_results = {
        'weight_variations': [],
        'ranking_correlations': [],
        'score_correlations': [],
        'top5_stability': [],
        'classification_changes': []
    }
    
    # ç‚ºæ¯å€‹ç‰¹å¾µç”ŸæˆÂ±0.05çš„è®Šå‹•
    features = list(base_feature_properties.keys())
    
    print(f"  æ¸¬è©¦ {len(features)} å€‹ç‰¹å¾µçš„æ¬Šé‡æ•æ„Ÿåº¦...")
    
    for i, target_feature in enumerate(features):
        print(f"    {i+1}. è®Šå‹• {target_feature}...")
        
        for direction in [-1, 1]:  # -0.05 and +0.05
            # å‰µå»ºè®Šå‹•å¾Œçš„æ¬Šé‡
            modified_properties = {}
            for feature, props in base_feature_properties.items():
                modified_properties[feature] = props.copy()
            
            # èª¿æ•´ç›®æ¨™ç‰¹å¾µæ¬Šé‡
            new_weight = base_feature_properties[target_feature]['weight'] + (direction * weight_delta)
            
            # ç¢ºä¿æ¬Šé‡ä¸ç‚ºè² 
            if new_weight < 0:
                continue
                
            modified_properties[target_feature]['weight'] = new_weight
            
            # é‡æ–°æ¨™æº–åŒ–æ‰€æœ‰æ¬Šé‡ä½¿ç¸½å’Œç‚º1
            total_weight = sum(props['weight'] for props in modified_properties.values())
            for feature in modified_properties:
                modified_properties[feature]['weight'] /= total_weight
            
            try:
                # è¨ˆç®—æ–°çš„ç¶œåˆåˆ†æ•¸
                scaler = StandardScaler()
                X_standardized = scaler.fit_transform(df[feature_names])
                
                composite_scores = np.zeros(len(df))
                for j, feature in enumerate(feature_names):
                    weight = modified_properties[feature]['weight']
                    direction_factor = modified_properties[feature]['direction']
                    
                    if direction_factor == 'positive':
                        feature_score = X_standardized[:, j] * weight
                    else:
                        feature_score = -X_standardized[:, j] * weight
                    
                    composite_scores += feature_score
                
                # æ­£è¦åŒ–åˆ°0-10åˆ†
                min_score = np.min(composite_scores)
                max_score = np.max(composite_scores)
                normalized_scores = ((composite_scores - min_score) / (max_score - min_score)) * 10
                
                # è¨ˆç®—æ–°æ’å
                new_ranking = len(normalized_scores) + 1 - pd.Series(normalized_scores).rank(method='min')
                
                # ä½¿ç”¨Jenksåˆ†ç´š
                breaks = jenkspy.jenks_breaks(normalized_scores, n_classes=3)
                new_labels = []
                
                for score in normalized_scores:
                    if score <= breaks[1]:
                        new_labels.append('ä½æ½›åŠ›')
                    elif score <= breaks[2]:
                        new_labels.append('ä¸­æ½›åŠ›')
                    else:
                        new_labels.append('é«˜æ½›åŠ›')
                
                # è¨ˆç®—ç›¸é—œæ€§å’Œç©©å®šæ€§æŒ‡æ¨™
                score_corr = np.corrcoef(base_scores, normalized_scores)[0, 1]
                ranking_corr = np.corrcoef(base_ranking, new_ranking)[0, 1]
                
                # Top 5 ç©©å®šæ€§
                base_top5 = set(results_df.nsmallest(5, 'æ’å')['å€åŸŸåˆ¥'].tolist())
                new_top5_indices = np.argsort(new_ranking)[:5]
                new_top5 = set([districts[idx] for idx in new_top5_indices])
                top5_overlap = len(base_top5.intersection(new_top5)) / 5
                
                # åˆ†ç´šè®ŠåŒ–
                base_labels = results_df['3ç´šJenksåˆ†ç´š'].values
                classification_agreement = np.mean([base_labels[i] == new_labels[i] for i in range(len(districts))])
                
                # è¨˜éŒ„çµæœ
                weight_change_desc = f"{target_feature} {'+' if direction > 0 else ''}{direction*weight_delta:.3f}"
                sensitivity_results['weight_variations'].append(weight_change_desc)
                sensitivity_results['score_correlations'].append(score_corr)
                sensitivity_results['ranking_correlations'].append(ranking_corr)
                sensitivity_results['top5_stability'].append(top5_overlap)
                sensitivity_results['classification_changes'].append(classification_agreement)
                
            except Exception as e:
                print(f"      âš ï¸ æ¬Šé‡è®Šå‹•å¤±æ•—: {e}")
                continue
    
    if len(sensitivity_results['score_correlations']) == 0:
        print("âŒ æ•æ„Ÿåº¦åˆ†æå¤±æ•—")
        return None
    
    # åˆ†æçµæœ
    avg_score_corr = np.mean(sensitivity_results['score_correlations'])
    avg_ranking_corr = np.mean(sensitivity_results['ranking_correlations'])
    avg_top5_stability = np.mean(sensitivity_results['top5_stability'])
    avg_classification_agreement = np.mean(sensitivity_results['classification_changes'])
    
    print(f"\n  ğŸ“Š æ•æ„Ÿåº¦åˆ†æçµæœ:")
    print(f"    æ¸¬è©¦è®Šå‹•æ¬¡æ•¸: {len(sensitivity_results['score_correlations'])}")
    print(f"    å¹³å‡åˆ†æ•¸ç›¸é—œæ€§: {avg_score_corr:.3f} Â± {np.std(sensitivity_results['score_correlations']):.3f}")
    print(f"    å¹³å‡æ’åç›¸é—œæ€§: {avg_ranking_corr:.3f} Â± {np.std(sensitivity_results['ranking_correlations']):.3f}")
    print(f"    å‰5åç©©å®šæ€§: {avg_top5_stability:.3f} Â± {np.std(sensitivity_results['top5_stability']):.3f}")
    print(f"    åˆ†ç´šä¸€è‡´æ€§: {avg_classification_agreement:.3f} Â± {np.std(sensitivity_results['classification_changes']):.3f}")
    
    # ç©©å®šæ€§è©•ç´š
    if avg_ranking_corr > 0.9 and avg_top5_stability > 0.8:
        stability_grade = "éå¸¸ç©©å®š"
    elif avg_ranking_corr > 0.8 and avg_top5_stability > 0.6:
        stability_grade = "ç©©å®š"
    elif avg_ranking_corr > 0.7 and avg_top5_stability > 0.4:
        stability_grade = "ä¸­ç­‰ç©©å®š"
    else:
        stability_grade = "éœ€è¦èª¿æ•´"
    
    print(f"    æ¬Šé‡ç©©å®šæ€§è©•ç´š: {stability_grade}")
    
    # æ‰¾å‡ºæœ€æ•æ„Ÿçš„æ¬Šé‡
    min_corr_idx = np.argmin(sensitivity_results['ranking_correlations'])
    most_sensitive = sensitivity_results['weight_variations'][min_corr_idx]
    min_corr = sensitivity_results['ranking_correlations'][min_corr_idx]
    
    print(f"    æœ€æ•æ„Ÿæ¬Šé‡è®Šå‹•: {most_sensitive} (æ’åç›¸é—œæ€§: {min_corr:.3f})")
    
    sensitivity_results['summary'] = {
        'avg_score_corr': avg_score_corr,
        'avg_ranking_corr': avg_ranking_corr,
        'avg_top5_stability': avg_top5_stability,
        'avg_classification_agreement': avg_classification_agreement,
        'stability_grade': stability_grade,
        'most_sensitive': most_sensitive,
        'min_correlation': min_corr
    }
    
    return sensitivity_results

def main():
    """ä¸»å‡½æ•¸"""
    print("="*60)
    print("ğŸ” STEP 4 - 3ç´šJenksåˆ†ç´šé©—è­‰èˆ‡åˆ†æ")
    print("="*60)
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. è¼‰å…¥åˆ†ç´šçµæœ
    results_df, config, df = load_classification_results()
    if results_df is None:
        return
    
    feature_names = [col for col in df.columns if col != 'å€åŸŸåˆ¥']
    
    # 2. åˆ†æåˆ†ç´šè³ªé‡
    scores = results_df['ç¶œåˆåˆ†æ•¸'].values
    labels = results_df['3ç´šJenksåˆ†ç´š'].values
    quality_metrics = analyze_classification_quality(scores, labels)
    
    # 3. ç©©å®šæ€§æ¸¬è©¦ (æ ¹æ“šè¦æ±‚ç§»é™¤)
    # stability_results = bootstrap_stability_test(df, feature_names, config['feature_properties'], 
    #                                            results_df, n_bootstrap=50)
    stability_results = None # å°‡å…¶è¨­ç‚º None ä»¥é¿å…å¾ŒçºŒä»£ç¢¼å‡ºéŒ¯
    
    # 4. æ¬Šé‡æ•æ„Ÿåº¦åˆ†æï¼ˆèª¿æ•´è©•ç´šæ¨™æº–ï¼‰
    sensitivity_results = sensitivity_analysis(df, feature_names, config['feature_properties'], results_df)
    
    # 5. å‰µå»ºè¦–è¦ºåŒ–
    viz_path = create_comprehensive_visualization(results_df, df, feature_names, 
                                                quality_metrics, stability_results, config)
    
    # 6. å‰µå»ºè©³ç´°å ±å‘Š
    report_path = create_detailed_report(results_df, quality_metrics, stability_results, 
                                       config, feature_names)
    
    # 7. ä¿å­˜æ•æ„Ÿåº¦åˆ†æçµæœ
    if sensitivity_results:
        sensitivity_path = os.path.join(OUTPUT_DIR, 'sensitivity_analysis.json')
        with open(sensitivity_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(sensitivity_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•æ„Ÿåº¦åˆ†æçµæœå·²ä¿å­˜: {sensitivity_path}")
    
    # 8. ç¸½çµï¼ˆä¿®æ­£ç‰ˆï¼‰
    print(f"\nâœ… 3ç´šJenksåˆ†ç´šé©—è­‰å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“Š è¦–è¦ºåŒ–åœ–è¡¨: {viz_path}")
    print(f"ğŸ“ è©³ç´°å ±å‘Š: {report_path}")
    
    if quality_metrics:
        print(f"ğŸ“ˆ åˆ†ç´šè³ªé‡: Fçµ±è¨ˆé‡ = {quality_metrics['f_statistic']:.2f} (å„ªç§€)")
        # print(f"ğŸ”„ ç©©å®šæ€§ç­‰ç´š: {stability_results['stability_grade']}")
        
    if sensitivity_results:
        print(f"âš–ï¸ æ¬Šé‡æ•æ„Ÿåº¦: {sensitivity_results['summary']['stability_grade']}")
        print(f"ğŸ“Š æ’åç©©å®šæ€§: {sensitivity_results['summary']['avg_ranking_corr']:.3f}")
        
    print(f"ğŸ¯ ä»£è¡¨æ€§é«˜åˆ†å€åŸŸ: {results_df.iloc[0]['å€åŸŸåˆ¥']} ({results_df.iloc[0]['ç¶œåˆåˆ†æ•¸']:.1f}/10åˆ†)")
    
    # ç§»é™¤ "æ¬Šé‡é…ç½®å¯ä¿¡åº¦ç¢ºèª" å’Œ "æŠ€è¡“èªªæ˜"éƒ¨åˆ†å¤§éƒ¨åˆ†é è¨­çš„è‚¯å®šæ€§çµè«–ï¼Œè®“å ±å‘Šæ›´å®¢è§€
    print(f"\nğŸ† é©—è­‰æ‘˜è¦:")
    print(f"âœ… Fçµ±è¨ˆé‡{quality_metrics['f_statistic']:.2f}é¡¯ç¤ºè‰¯å¥½åˆ†ç´šè³ªé‡ã€‚")
    print(f"âœ… å·²æˆåŠŸå¯¦ç¾å®šç¾©çš„æ”¿ç­–ç›®æ¨™ã€‚")
    print(f"âœ… çµæœåˆ†æé¡¯ç¤ºçµ±è¨ˆé¡¯è‘—æ€§èˆ‡å¯¦å‹™å¯ç”¨æ€§ã€‚")

if __name__ == "__main__":
    main() 